{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba73642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from os import error\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import glob\n",
    "import torch.distributions.multivariate_normal as torchdist\n",
    "\n",
    "from utils_expert import *\n",
    "from metrics import *\n",
    "from model_lstm import Goal_Example_Model\n",
    "from helper_expert import *\n",
    "import copy\n",
    "from gmm2d import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b44d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"zara1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944f17bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Expert:\n",
    "    def __init__(self, obs_traj_norm, velocity_obs, pred_traj_gt):\n",
    "        self.obs_traj_norm = obs_traj_norm\n",
    "        self.velocity_obs = velocity_obs\n",
    "        self.pred_traj_gt = pred_traj_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89593c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(KSTEPS=20, dataset_name=\"eth\", online_expert=True):\n",
    "    global loader_test, model, log_file_curve, dset_train, dset_val\n",
    "    model.eval()\n",
    "    expert_dest = None\n",
    "    saved_num_obj = 0\n",
    "\n",
    "    if not online_expert:\n",
    "        expert_dest = np.load(\n",
    "            \"./checkpoint_ethucy/test_{}_expert.npy\".format(dataset_name)\n",
    "        )\n",
    "        print(\"Loading stored expert examples test_{}_expert.npy\".format(dataset_name))\n",
    "\n",
    "        # [num_of_objs, 8, 2]\n",
    "        saved_num_obj = expert_dest.shape[0]\n",
    "        print(\"total number of expert data point is {}\".format(saved_num_obj))\n",
    "\n",
    "    ade_bigls = []\n",
    "    fde_bigls = []\n",
    "    raw_data_dict = {}\n",
    "    all_experts = []\n",
    "    step = 0  # I can use this step to track the experts\n",
    "    total_num_of_objs = 0\n",
    "    all_goal_error = []\n",
    "\n",
    "    for batch in loader_test:\n",
    "        step += 1\n",
    "\n",
    "        # Get data\n",
    "        batch = [tensor.cuda() for tensor in batch]\n",
    "\n",
    "        (\n",
    "            obs_traj_norm,\n",
    "            obs_traj,\n",
    "            obs_traj_rel,\n",
    "            pred_traj_gt,\n",
    "            pred_traj_gt_rel,\n",
    "            V_obs,\n",
    "            A_obs,\n",
    "            V_tr,\n",
    "            A_tr,\n",
    "            inp_mask,\n",
    "            out_mask,\n",
    "            velocity_obs,\n",
    "            velocity_pred,\n",
    "            acc_obs,\n",
    "            acc_pred,\n",
    "            seq_start,\n",
    "        ) = batch\n",
    "\n",
    "        \"\"\"\n",
    "        Perform the experties matching here\n",
    "        \"\"\"\n",
    "        num_of_objs = int(sum(inp_mask[0, 0]))\n",
    "\n",
    "        if online_expert:\n",
    "            data = Data_Expert(obs_traj_norm, velocity_obs, pred_traj_gt)\n",
    "            end_error, rst = expert_find(\n",
    "                data, num_of_objs, dset_train, dset_val, gamma=1.0\n",
    "            )\n",
    "            rst = torch.stack(rst)  # [num_of_objs, 2]\n",
    "            rst = rst.reshape(1, 1, num_of_objs, 2).repeat(1, 8, 1, 1)\n",
    "            end_error_list = [x.item() for x in end_error]\n",
    "            all_goal_error.append(\n",
    "                end_error_list\n",
    "            )  # list of all lowest goal error of num_of_objs,\n",
    "\n",
    "            all_experts.append(\n",
    "                rst.squeeze(0).permute(1, 0, 2).data.cpu().numpy()\n",
    "            )  # [num_of_objs, 8, 2]\n",
    "            print(\n",
    "                \"Averaging end-point error is {}\".format(sum(end_error) / num_of_objs)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            rst = expert_dest[total_num_of_objs : num_of_objs + total_num_of_objs]\n",
    "            rst = torch.from_numpy(rst).unsqueeze(0).cuda()\n",
    "            rst = rst.permute(0, 2, 1, 3)\n",
    "            total_num_of_objs += num_of_objs\n",
    "\n",
    "        rst = rst.view(1, 8, num_of_objs, 2)\n",
    "        obs_traj_norm[:, :, :num_of_objs] = obs_traj_norm[:, :, :num_of_objs] - (\n",
    "            rst / 1.0\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        Perform the regular inference process\n",
    "        \"\"\"\n",
    "        V_obs_tmp = torch.cat([obs_traj_norm, velocity_obs, acc_obs], dim=-1)\n",
    "        # V_obs_tmp = obs_traj_norm\n",
    "        V_obs_tmp = V_obs_tmp.permute(0, 3, 1, 2)\n",
    "\n",
    "        V_pred, _ = model(V_obs_tmp, A_obs, inp_mask, out_mask)\n",
    "\n",
    "        V_tr = V_tr.squeeze()\n",
    "        A_tr = A_tr.squeeze()\n",
    "        V_pred = V_pred.squeeze()\n",
    "        num_of_objs = int(sum(inp_mask[0, 0]))\n",
    "\n",
    "        # only evaluate on valid nodes;\n",
    "        V_pred, V_tr, obs_traj, obs_traj_rel, V_obs, pred_traj_gt = (\n",
    "            V_pred[:, :num_of_objs, :],\n",
    "            V_tr[:, :num_of_objs, :],\n",
    "            obs_traj.squeeze()[:, :num_of_objs, :],\n",
    "            obs_traj_rel.squeeze()[:, :num_of_objs, :],\n",
    "            V_obs.squeeze()[:, :num_of_objs, :],\n",
    "            pred_traj_gt.squeeze()[:, :num_of_objs, :],\n",
    "        )\n",
    "\n",
    "        log_pis = torch.ones(V_pred[..., -2:-1].shape)\n",
    "        gmm2d = GMM2D(\n",
    "            log_pis,\n",
    "            V_pred[..., 0:2],\n",
    "            V_pred[..., 2:4],\n",
    "            Func.tanh(V_pred[..., -1]).unsqueeze(-1),\n",
    "        )\n",
    "\n",
    "        # Now sample 20 samples\n",
    "        ade_ls = {}\n",
    "        fde_ls = {}\n",
    "\n",
    "        V_x = seq_to_nodes(obs_traj.data.cpu().numpy().copy())\n",
    "        V_x_rel_to_abs = nodes_rel_to_nodes_abs(\n",
    "            V_obs.data.cpu().numpy().copy(),\n",
    "            V_x[0, :, :].copy(),\n",
    "        )\n",
    "        \"\"\"For only one ped case\"\"\"\n",
    "        if len(V_x_rel_to_abs.shape) < 3:\n",
    "            V_x_rel_to_abs = np.expand_dims(V_x_rel_to_abs, 1)\n",
    "\n",
    "        V_y = seq_to_nodes(pred_traj_gt.data.cpu().numpy().copy())\n",
    "        V_y_rel_to_abs = nodes_rel_to_nodes_abs(\n",
    "            V_tr.data.cpu().numpy().copy(),\n",
    "            V_x[-1, :, :].copy(),\n",
    "        )\n",
    "\n",
    "        \"\"\"For only one ped case\"\"\"\n",
    "        if len(V_y_rel_to_abs.shape) < 3:\n",
    "            V_y_rel_to_abs = np.expand_dims(V_y_rel_to_abs, 1)\n",
    "\n",
    "        raw_data_dict[step] = {}\n",
    "        raw_data_dict[step][\"obs\"] = copy.deepcopy(V_x_rel_to_abs)\n",
    "        raw_data_dict[step][\"trgt\"] = copy.deepcopy(V_y_rel_to_abs)\n",
    "        raw_data_dict[step][\"pred\"] = []\n",
    "\n",
    "        for n in range(num_of_objs):\n",
    "            ade_ls[n] = []\n",
    "            fde_ls[n] = []\n",
    "\n",
    "        for k in range(KSTEPS):\n",
    "            V_pred = gmm2d.rsample()\n",
    "\n",
    "            \"\"\"Evaluate rel output\"\"\"\n",
    "            V_pred_rel_to_abs = nodes_rel_to_nodes_abs(\n",
    "                V_pred.data.cpu().numpy().copy(),\n",
    "                V_x[-1, :, :].copy(),\n",
    "            )\n",
    "\n",
    "            \"\"\"For only one ped case\"\"\"\n",
    "            if len(V_pred_rel_to_abs.shape) < 3:\n",
    "                V_pred_rel_to_abs = np.expand_dims(V_pred_rel_to_abs, 1)\n",
    "\n",
    "            \"\"\"Plug rst into the last position of V_y_rel_to_abs\n",
    "            Thus, use the retrieved goal to do the evaluation;\n",
    "            \n",
    "            Comment out for refinement;\n",
    "            \"\"\"\n",
    "            V_pred_rel_to_abs[-1] = rst[0, -1].cpu().numpy()\n",
    "\n",
    "            raw_data_dict[step][\"pred\"].append(copy.deepcopy(V_pred_rel_to_abs))\n",
    "\n",
    "            for n in range(num_of_objs):\n",
    "                pred = []\n",
    "                target = []\n",
    "                obsrvs = []\n",
    "                number_of = []\n",
    "                pred.append(V_pred_rel_to_abs[:, n : n + 1, :])\n",
    "                target.append(V_y_rel_to_abs[:, n : n + 1, :])\n",
    "                obsrvs.append(V_x_rel_to_abs[:, n : n + 1, :])\n",
    "                number_of.append(1)\n",
    "\n",
    "                ade_ls[n].append(ade(pred, target, number_of))\n",
    "                fde_ls[n].append(fde(pred, target, number_of))\n",
    "\n",
    "        for n in range(num_of_objs):\n",
    "            ade_bigls.append(min(ade_ls[n]))\n",
    "            fde_bigls.append(min(fde_ls[n]))\n",
    "\n",
    "    print(total_num_of_objs)\n",
    "    ade_ = sum(ade_bigls) / len(ade_bigls)\n",
    "    fde_ = sum(fde_bigls) / len(fde_bigls)\n",
    "    return ade_, fde_, all_experts, all_goal_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d6e27bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Number of samples: 20\n",
      "**************************************************\n",
      "State of online_expert : False\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "paths = \"./checkpoint_ethucy/{}_best.pth\".format(dataset_name)\n",
    "\n",
    "\n",
    "KSTEPS = 20\n",
    "grad_eff = 0.4\n",
    "load_expert_local = True\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print(\"Number of samples:\", KSTEPS)\n",
    "print(\"*\" * 50)\n",
    "NUM_PED_ALL = 0.0\n",
    "online_expert = False\n",
    "print(\"State of online_expert : {}\".format(online_expert))\n",
    "\n",
    "\n",
    "ade_ls = []\n",
    "fde_ls = []\n",
    "\n",
    "print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb255e",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ffe692d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 705/705 [00:00<00:00, 1287.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data prep\n",
    "obs_seq_len = 8\n",
    "pred_seq_len = 12\n",
    "data_set = \"./datasets/\" + dataset_name + \"/\"\n",
    "\n",
    "dset_test = TrajectoryDataset(\n",
    "    data_set + \"test/\",\n",
    "    obs_len=obs_seq_len,\n",
    "    pred_len=pred_seq_len,\n",
    "    skip=1,\n",
    "    norm_lap_matr=True,\n",
    "    grad_eff=grad_eff,\n",
    ")\n",
    "\n",
    "if online_expert:\n",
    "    if load_expert_local and os.path.exists(\n",
    "        \"/media/chris/hdd1/expert_traj/{}_expert_train_{}.pth\".format(\n",
    "            dataset_name, grad_eff\n",
    "        )\n",
    "    ):\n",
    "        with open(\n",
    "            \"/media/chris/hdd1/expert_traj/{}_expert_train_{}.pth\".format(\n",
    "                dataset_name, grad_eff\n",
    "            ),\n",
    "            \"rb\",\n",
    "        ) as f:\n",
    "            dset_train = pickle.load(f)\n",
    "    else:\n",
    "        dset_train = TrajectoryDataset(\n",
    "            data_set + \"train/\",\n",
    "            obs_len=obs_seq_len,\n",
    "            pred_len=pred_seq_len,\n",
    "            skip=1,\n",
    "            norm_lap_matr=True,\n",
    "            grad_eff=grad_eff,\n",
    "        )\n",
    "else:\n",
    "    dset_train = None\n",
    "\n",
    "if online_expert:\n",
    "    if load_expert_local and os.path.exists(\n",
    "        \"/media/chris/hdd1/expert_traj/{}_expert_val_{}.pth\".format(\n",
    "            dataset_name, grad_eff\n",
    "        )\n",
    "    ):\n",
    "        with open(\n",
    "            \"/media/chris/hdd1/expert_traj/{}_expert_val_{}.pth\".format(\n",
    "                dataset_name, grad_eff\n",
    "            ),\n",
    "            \"rb\",\n",
    "        ) as f:\n",
    "            dset_val = pickle.load(f)\n",
    "    else:\n",
    "        dset_val = TrajectoryDataset(\n",
    "            data_set + \"val/\",\n",
    "            obs_len=obs_seq_len,\n",
    "            pred_len=pred_seq_len,\n",
    "            skip=1,\n",
    "            norm_lap_matr=True,\n",
    "            grad_eff=grad_eff,\n",
    "        )\n",
    "else:\n",
    "    dset_val = None\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    dset_test,\n",
    "    batch_size=1,  # This is irrelative to the args batch size parameter\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "\"\"\"Save augmented expert to local \"\"\"\n",
    "save_expert_local = False\n",
    "\n",
    "if save_expert_local:\n",
    "\n",
    "    if not os.path.exists(\"./{}_expert_train_{}.pth\".format(dataset_name, grad_eff)):\n",
    "        with open(\n",
    "            \"./{}_expert_train_{}.pth\".format(dataset_name, grad_eff),\n",
    "            \"wb\",\n",
    "        ) as f:\n",
    "            pickle.dump(dset_train, f)\n",
    "\n",
    "    if not os.path.exists(\"./{}_expert_val_{}.pth\".format(dataset_name, grad_eff)):\n",
    "        with open(\n",
    "            \"./{}_expert_val_{}.pth\".format(dataset_name, grad_eff),\n",
    "            \"wb\",\n",
    "        ) as f:\n",
    "            pickle.dump(dset_val, f)\n",
    "        print(\"Saving two expert examples for dataset {}\".format(dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e6e5ce",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc0f1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Goal_Example_Model(\n",
       "  (st_gcns): ModuleList(\n",
       "    (0): st_gcn(\n",
       "      (bn1): LayerNorm((8, 128), eps=1e-05, elementwise_affine=True)\n",
       "      (gcn): ConvTemporalGraphical(\n",
       "        (conv): Conv2d(6, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (bn0): LayerNorm((8, 128), eps=1e-05, elementwise_affine=True)\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (tcn): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "      (dropout): Dropout(p=0, inplace=True)\n",
       "      (residual): Conv2d(6, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): LayerNorm((8, 128), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (enc_lstm): LSTM(128, 128)\n",
       "  (state_start): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (dec_lstm): LSTM(130, 128)\n",
       "  (out_mus): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (out_sigma): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (out_corr): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the model\n",
    "model = Goal_Example_Model(\n",
    "    n_stgcnn=1,\n",
    "    n_txpcnn=5,\n",
    "    input_feat=6,\n",
    "    output_feat=128,\n",
    "    seq_len=8,\n",
    "    kernel_size=3,\n",
    "    pred_seq_len=12,\n",
    ").cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c5109",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6d4fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating epoch ./checkpoint_ethucy/zara1_best.pth\n",
      "Testing ....\n",
      "Loading stored expert examples test_zara1_expert.npy\n",
      "total number of expert data point is 2356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dani/.virtualenvs/VAA/lib/python3.8/site-packages/torch/distributions/distribution.py:44: UserWarning: <class 'gmm2d.GMM2D'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n",
      "/home/dani/.virtualenvs/VAA/lib/python3.8/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356\n",
      "ADE: 0.11125941823609305  FDE: 0.31064109908156856\n"
     ]
    }
   ],
   "source": [
    "model_paths = glob.glob(paths)\n",
    "\n",
    "for num_avg in range(1):\n",
    "    for model_path in model_paths:\n",
    "        print(\"evaluating epoch {}\".format(model_path))\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        ade_ = 999999\n",
    "        fde_ = 999999\n",
    "        print(\"Testing ....\")\n",
    "\n",
    "        ad, fd, all_experts, goal_errors = test(\n",
    "            20, dataset_name=dataset_name, online_expert=online_expert\n",
    "        )\n",
    "\n",
    "        if online_expert:\n",
    "            with open(\n",
    "                os.path.join(\"test_{}_expert.npy\".format(dataset_name)), \"wb\"\n",
    "            ) as f:\n",
    "                np.save(f, np.concatenate(all_experts, 0))\n",
    "\n",
    "            with open(\n",
    "                os.path.join(\"{}_expert_goal_error.npy\".format(dataset_name)),\n",
    "                \"wb\",\n",
    "            ) as f:\n",
    "                np.save(f, np.concatenate(goal_errors, 0))\n",
    "\n",
    "            data = np.concatenate(goal_errors, 0)\n",
    "            print(data.mean())\n",
    "\n",
    "        ade_ = min(ade_, ad)\n",
    "        fde_ = min(fde_, fd)\n",
    "        ade_ls.append(ade_)\n",
    "        fde_ls.append(fde_)\n",
    "    print(\n",
    "        \"ADE:\",\n",
    "        min(ade_ls),\n",
    "        \" FDE:\",\n",
    "        min(fde_ls),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554cd657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
