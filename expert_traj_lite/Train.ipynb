{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e1a1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "\n",
    "from social_utils import *\n",
    "import yaml\n",
    "\n",
    "from model_sdd import Goal_example_model\n",
    "import numpy as np\n",
    "import pdb\n",
    "from gmm2d import *\n",
    "\n",
    "from metrics import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d32b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class GoalExample:\n",
    "    num_workers: int \n",
    "    gpu_index: int\n",
    "    config_filename: str \n",
    "    save_file: str \n",
    "    verbose: bool\n",
    "    lr: float\n",
    "    input_feat: int\n",
    "    output_feat: int \n",
    "    checkpoint: str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d0e88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = GoalExample(0, 0, \"optimal.yaml\", \"PECNET_social_model.pt\", True, 0.0003, 2, 128, \"./checkpoint_sdd_abs2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b4bc899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "args.checkpoint = \"./sdd_wo_goal\"\n",
    "\n",
    "dtype = torch.float64\n",
    "\n",
    "torch.set_default_dtype(dtype)\n",
    "device = (\n",
    "    torch.device(\"cuda\", index=args.gpu_index)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(args.gpu_index)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "443d205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_bivariate_loss_ssd(V_pred, V_trgt):\n",
    "    \"\"\"\n",
    "    V_pred, V_trgt:\n",
    "        [Batch, Seq_len, Nodes, 5/2];\n",
    "\n",
    "    \"\"\"\n",
    "    # mux, muy, sx, sy, corr\n",
    "    # assert V_pred.shape == V_trgt.shape\n",
    "    normx = V_trgt[..., 0] - V_pred[..., 0]\n",
    "    normy = V_trgt[..., 1] - V_pred[..., 1]\n",
    "\n",
    "    sx = torch.exp(V_pred[..., 2])  # sx\n",
    "    sy = torch.exp(V_pred[..., 3])  # sy\n",
    "    corr = torch.tanh(V_pred[..., 4])  # corr\n",
    "\n",
    "    sxsy = sx * sy\n",
    "\n",
    "    z = (normx / sx) ** 2 + (normy / sy) ** 2 - 2 * ((corr * normx * normy) / sxsy)\n",
    "    negRho = 1 - corr ** 2\n",
    "\n",
    "    # Numerator\n",
    "    result = torch.exp(-z / (2 * negRho))\n",
    "    # Normalization factor\n",
    "    denom = 2 * np.pi * (sxsy * torch.sqrt(negRho))\n",
    "\n",
    "    # Final PDF calculation\n",
    "    result = result / denom\n",
    "\n",
    "    # Numerical stability\n",
    "    epsilon = 1e-20\n",
    "\n",
    "    result = -torch.log(torch.clamp(result, min=epsilon))\n",
    "\n",
    "    return result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e8b8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_loss(V_pred, V_target):\n",
    "    return batch_bivariate_loss_ssd(V_pred, V_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "464f5564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adl_reg': 1, 'data_scale': 1.86, 'dataset_type': 'image', 'dec_size': [1024, 512, 1024], 'dist_thresh': 100, 'enc_dest_size': [8, 16], 'enc_latent_size': [8, 50], 'enc_past_size': [512, 256], 'non_local_theta_size': [256, 128, 64], 'non_local_phi_size': [256, 128, 64], 'non_local_g_size': [256, 128, 64], 'non_local_dim': 128, 'fdim': 16, 'future_length': 12, 'gpu_index': 0, 'kld_reg': 1, 'learning_rate': 0.0003, 'mu': 0, 'n_values': 20, 'nonlocal_pools': 3, 'normalize_type': 'shift_origin', 'num_epochs': 650, 'num_workers': 0, 'past_length': 8, 'predictor_hidden_size': [1024, 512, 256], 'sigma': 1.3, 'test_b_size': 4096, 'time_thresh': 0, 'train_b_size': 512, 'zdim': 16}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./config/\" + args.config_filename, \"r\") as file:\n",
    "    try:\n",
    "        hyper_params = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    except:\n",
    "        hyper_params = yaml.load(file)\n",
    "file.close()\n",
    "print(hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "877dfb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./social_pool_data/train_all_512_0_100.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dani/Desktop/Pred/expert_traj_lite/social_utils.py:280: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  traj_new = np.array(traj_new)\n",
      "/home/dani/Desktop/Pred/expert_traj_lite/social_utils.py:281: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  masks_new = np.array(masks_new)\n",
      "/home/dani/Desktop/Pred/expert_traj_lite/social_utils.py:287: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.initial_pos_batches = np.array(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized social dataloader...\n",
      "./social_pool_data/test_all_4096_0_100.pickle\n",
      "Initialized social dataloader...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SocialDataset(\n",
    "    set_name=\"train\",\n",
    "    b_size=hyper_params[\"train_b_size\"],\n",
    "    t_tresh=hyper_params[\"time_thresh\"],\n",
    "    d_tresh=hyper_params[\"dist_thresh\"],\n",
    "    verbose=args.verbose,\n",
    ")\n",
    "\n",
    "test_dataset = SocialDataset(\n",
    "    set_name=\"test\",\n",
    "    b_size=hyper_params[\"test_b_size\"],\n",
    "    t_tresh=hyper_params[\"time_thresh\"],\n",
    "    d_tresh=hyper_params[\"dist_thresh\"],\n",
    "    verbose=args.verbose,\n",
    ")\n",
    "\n",
    "model = Goal_example_model(\n",
    "    input_feat=args.input_feat,\n",
    "    output_feat=args.output_feat,\n",
    "    config=hyper_params,\n",
    "    non_local_loop=0,\n",
    ").cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ec62605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare some data for this batch of data\"\"\"\n",
    "# shift origin and scale data\n",
    "for traj in train_dataset.trajectory_batches:\n",
    "    traj -= traj[:, :1, :]\n",
    "    traj *= 0.2\n",
    "\n",
    "for traj in test_dataset.trajectory_batches:\n",
    "    traj -= traj[:, :1, :]\n",
    "    traj *= 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755a89a",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a822d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataset, best_of_n=20):\n",
    "    global model, optim\n",
    "    model.eval()\n",
    "    ade_bigls = []\n",
    "    fde_bigls = []\n",
    "\n",
    "    for i, (traj, mask, initial_pos) in enumerate(\n",
    "        zip(\n",
    "            test_dataset.trajectory_batches,\n",
    "            test_dataset.mask_batches,\n",
    "            test_dataset.initial_pos_batches,\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        traj_v = np.gradient(np.transpose(traj, (0, 2, 1)), 0.4, axis=-1)\n",
    "        traj_a = np.gradient(traj_v, 0.4, axis=-1)\n",
    "        traj_v = torch.from_numpy(traj_v).permute(0, 2, 1)\n",
    "        traj_a = torch.from_numpy(traj_a).permute(0, 2, 1)\n",
    "\n",
    "        traj, mask, initial_pos, traj_a, traj_v = (\n",
    "            torch.DoubleTensor(traj).to(device),\n",
    "            torch.DoubleTensor(mask).to(device),\n",
    "            torch.DoubleTensor(initial_pos).to(device),\n",
    "            torch.DoubleTensor(traj_a).to(device),\n",
    "            torch.DoubleTensor(traj_v).to(device),\n",
    "        )\n",
    "\n",
    "        \"\"\"Pre-process data into relative coords\"\"\"\n",
    "        # input_traj = traj[:, : hyper_params[\"past_length\"], :]\n",
    "        dest = traj[:, -1].unsqueeze(1).repeat(1, 8, 1)\n",
    "        # dest = 0.0\n",
    "        # dest = torch.mean(traj, 1).unsqueeze(1).repeat(1, 8, 1)\n",
    "        # input_traj = torch.cat(\n",
    "        #     [\n",
    "        #         traj[:, : hyper_params[\"past_length\"]] - (dest / 3.0),\n",
    "        #         traj_v[:, : hyper_params[\"past_length\"]],\n",
    "        #         traj_a[:, : hyper_params[\"past_length\"]],\n",
    "        #     ],\n",
    "        #     -1,\n",
    "        # )\n",
    "        # input_traj = traj[:, : hyper_params[\"past_length\"]] - (dest / 2.0)\n",
    "        input_traj = traj[:, : hyper_params[\"past_length\"]] - (dest)\n",
    "        # input_traj = traj[:, : hyper_params[\"past_length\"]] - dest\n",
    "        # input_traj = torch.cat([traj[:, : hyper_params[\"past_length\"]], dest[:, :1]], 1)\n",
    "\n",
    "        init_traj = traj[\n",
    "            :, hyper_params[\"past_length\"] - 1 : hyper_params[\"past_length\"], :\n",
    "        ]\n",
    "        V_tr = traj[:, hyper_params[\"past_length\"] :, :]\n",
    "\n",
    "        V_pred, _ = model(input_traj, mask)\n",
    "        V_pred = V_pred.squeeze()\n",
    "\n",
    "        log_pis = torch.ones(V_pred[..., -2:-1].shape)\n",
    "        gmm2d = GMM2D(\n",
    "            log_pis,\n",
    "            V_pred[..., 0:2],\n",
    "            V_pred[..., 2:4],\n",
    "            Func.tanh(V_pred[..., -1]).unsqueeze(-1),\n",
    "        )\n",
    "\n",
    "        ade_ls = {}\n",
    "        fde_ls = {}\n",
    "        for n in range(traj.shape[0]):\n",
    "            ade_ls[n] = []\n",
    "            fde_ls[n] = []\n",
    "\n",
    "        for k in range(best_of_n):\n",
    "            V_pred = gmm2d.rsample().squeeze()\n",
    "\n",
    "            \"\"\"Evaluate rel output\n",
    "\n",
    "            Comment out for evaluating abs output\n",
    "            \"\"\"\n",
    "            # V_pred = torch.cumsum(V_pred, dim=1) + init_traj.repeat(1, 12, 1)\n",
    "\n",
    "            for n in range(traj.shape[0]):\n",
    "                ade_ls[n].append(torch.norm(V_pred[n] - V_tr[n], dim=-1).mean())\n",
    "                fde_ls[n].append(torch.norm(V_pred[n, -1] - V_tr[n, -1]))\n",
    "\n",
    "        # Metrics\n",
    "        for n in range(traj.shape[0]):\n",
    "            ade_bigls.append(min(ade_ls[n]))\n",
    "            fde_bigls.append(min(fde_ls[n]))\n",
    "\n",
    "    ade_ = sum(ade_bigls) / len(ade_bigls)\n",
    "    fde_ = sum(fde_bigls) / len(fde_bigls)\n",
    "    return ade_, fde_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e81521",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d3d2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, epoch):\n",
    "    global model, optim\n",
    "    model.train()\n",
    "\n",
    "    for i, (traj, mask, initial_pos) in enumerate(\n",
    "        zip(\n",
    "            train_dataset.trajectory_batches,\n",
    "            train_dataset.mask_batches,\n",
    "            train_dataset.initial_pos_batches,\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        traj_v = np.gradient(np.transpose(traj, (0, 2, 1)), 0.4, axis=-1)\n",
    "        traj_a = np.gradient(traj_v, 0.4, axis=-1)\n",
    "        traj_v = torch.from_numpy(traj_v).permute(0, 2, 1)\n",
    "        traj_a = torch.from_numpy(traj_a).permute(0, 2, 1)\n",
    "\n",
    "        traj, mask, initial_pos, traj_v, traj_a = (\n",
    "            torch.DoubleTensor(traj).to(device),\n",
    "            torch.DoubleTensor(mask).to(device),\n",
    "            torch.DoubleTensor(initial_pos).to(device),\n",
    "            torch.DoubleTensor(traj_v).to(device),\n",
    "            torch.DoubleTensor(traj_a).to(device),\n",
    "        )\n",
    "\n",
    "        \"\"\"Pre-process data into relative coords\"\"\"\n",
    "        # rel_traj = traj[:, 1:] - traj[:, :-1]\n",
    "        # V_tr = rel_traj[:, -12:]\n",
    "        V_tr = traj[:, hyper_params[\"past_length\"] :]\n",
    "        dest = traj[:, -1].unsqueeze(1).repeat(1, 8, 1)\n",
    "        # dest = 0.0\n",
    "        # dest = torch.mean(traj, 1).unsqueeze(1).repeat(1, 8, 1)\n",
    "\n",
    "        # input_traj = torch.cat(\n",
    "        # [\n",
    "        # traj[:, : hyper_params[\"past_length\"]] - (dest / 3.0),\n",
    "        # traj_a[:, : hyper_params[\"past_length\"]],\n",
    "        # traj_v[:, : hyper_params[\"past_length\"]],\n",
    "        # ],\n",
    "        # -1,\n",
    "        # )\n",
    "\n",
    "        # input_traj = traj[:, : hyper_params[\"past_length\"]] - (dest / 2.0)\n",
    "        input_traj = traj[:, : hyper_params[\"past_length\"]] - (dest)\n",
    "        # input_traj = traj[:, : hyper_params[\"past_length\"]] - dest\n",
    "        # input_traj = torch.cat([traj[:, : hyper_params[\"past_length\"]], dest[:, :1]], 1)\n",
    "\n",
    "        V_pred, _ = model(input_traj, mask)\n",
    "        V_pred = V_pred.squeeze()\n",
    "\n",
    "        loss = graph_loss(V_pred, V_tr)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        loss_batch = loss.item()\n",
    "        print(\"TRAIN:\", \"\\t Epoch:\", epoch, \"\\t Loss:\", loss_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699c9ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294dfef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 0 \t Loss: 4.976104639542424\n",
      "TRAIN: \t Epoch: 0 \t Loss: 4.119008614846586\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.17392846187933\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.268663329851114\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.317826901853049\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.231563614459357\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.702281257661526\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.652488873885025\n",
      "TRAIN: \t Epoch: 0 \t Loss: 4.244416648440456\n",
      "TRAIN: \t Epoch: 0 \t Loss: 4.272580113987758\n",
      "TRAIN: \t Epoch: 0 \t Loss: 4.55361896224454\n",
      "TRAIN: \t Epoch: 0 \t Loss: 4.416958872445815\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.488948156736695\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.518836780830007\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.6975577504131865\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.796957234079277\n",
      "TRAIN: \t Epoch: 0 \t Loss: 7.236629328871521\n",
      "TRAIN: \t Epoch: 0 \t Loss: 7.165961565620172\n",
      "TRAIN: \t Epoch: 0 \t Loss: 7.506950978945658\n",
      "TRAIN: \t Epoch: 0 \t Loss: 7.442430360161001\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.907208124759866\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.95904282533692\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.035491897745939\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.96423113972236\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.788870847209887\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.7236790577357874\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.580532162442734\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.753579416145186\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.672634281343353\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.527571553288568\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.595958340219923\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.662118150770249\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.444233651119956\n",
      "TRAIN: \t Epoch: 0 \t Loss: 6.491646716955765\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.338370103709281\n",
      "TRAIN: \t Epoch: 0 \t Loss: 5.376706164274585\n",
      "TRAIN: \t Epoch: 1 \t Loss: 4.029136206725682\n",
      "TRAIN: \t Epoch: 1 \t Loss: 3.944074730724344\n",
      "TRAIN: \t Epoch: 1 \t Loss: 4.6441829166416495\n",
      "TRAIN: \t Epoch: 1 \t Loss: 4.6825003175257125\n",
      "TRAIN: \t Epoch: 1 \t Loss: 4.762159877355609\n",
      "TRAIN: \t Epoch: 1 \t Loss: 4.66918090490343\n",
      "TRAIN: \t Epoch: 1 \t Loss: 5.106147438770035\n",
      "TRAIN: \t Epoch: 1 \t Loss: 5.082112608105306\n",
      "TRAIN: \t Epoch: 1 \t Loss: 3.3627694809647815\n",
      "TRAIN: \t Epoch: 1 \t Loss: 3.4949825436115427\n",
      "TRAIN: \t Epoch: 1 \t Loss: 3.892260030443699\n",
      "TRAIN: \t Epoch: 1 \t Loss: 3.8312844279451608\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.189129627834817\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.063387663443636\n",
      "TRAIN: \t Epoch: 1 \t Loss: 5.260116833047716\n",
      "TRAIN: \t Epoch: 1 \t Loss: 5.391697450995343\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.811686997863944\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.76468838656037\n",
      "TRAIN: \t Epoch: 1 \t Loss: 7.258195191069502\n",
      "TRAIN: \t Epoch: 1 \t Loss: 7.163490611089527\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.47884527688573\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.527869383909611\n",
      "TRAIN: \t Epoch: 1 \t Loss: 5.80025008329901\n",
      "TRAIN: \t Epoch: 1 \t Loss: 5.7315239331994015\n",
      "TRAIN: \t Epoch: 1 \t Loss: 5.450838289723681\n",
      "TRAIN: \t Epoch: 1 \t Loss: 5.355197170288337\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.357286744790022\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.469587863817646\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.349840818601939\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.3016216741173725\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.387416353543256\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.369756575829141\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.24724485959761\n",
      "TRAIN: \t Epoch: 1 \t Loss: 6.320303644535357\n",
      "TRAIN: \t Epoch: 1 \t Loss: 5.0378633947792535\n",
      "TRAIN: \t Epoch: 1 \t Loss: 5.10951586911229\n",
      "TRAIN: \t Epoch: 2 \t Loss: 3.5183267601465267\n",
      "TRAIN: \t Epoch: 2 \t Loss: 3.5478801806849964\n",
      "TRAIN: \t Epoch: 2 \t Loss: 4.34972341993415\n",
      "TRAIN: \t Epoch: 2 \t Loss: 4.407728216739828\n",
      "TRAIN: \t Epoch: 2 \t Loss: 4.411957678199472\n",
      "TRAIN: \t Epoch: 2 \t Loss: 4.361790979667381\n",
      "TRAIN: \t Epoch: 2 \t Loss: 4.800106274825258\n",
      "TRAIN: \t Epoch: 2 \t Loss: 4.770130615887651\n",
      "TRAIN: \t Epoch: 2 \t Loss: 3.1165949390941208\n",
      "TRAIN: \t Epoch: 2 \t Loss: 3.241107897186219\n",
      "TRAIN: \t Epoch: 2 \t Loss: 3.7613591498298393\n",
      "TRAIN: \t Epoch: 2 \t Loss: 3.6802837623249016\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.014045774983107\n",
      "TRAIN: \t Epoch: 2 \t Loss: 5.921448115938869\n",
      "TRAIN: \t Epoch: 2 \t Loss: 5.093447992142783\n",
      "TRAIN: \t Epoch: 2 \t Loss: 5.241332611157193\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.691007639430798\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.616411607914796\n",
      "TRAIN: \t Epoch: 2 \t Loss: 7.072255393604238\n",
      "TRAIN: \t Epoch: 2 \t Loss: 7.021008302923427\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.214530626528322\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.270422398312279\n",
      "TRAIN: \t Epoch: 2 \t Loss: 5.724004624775308\n",
      "TRAIN: \t Epoch: 2 \t Loss: 5.6326980138135285\n",
      "TRAIN: \t Epoch: 2 \t Loss: 5.347303324285623\n",
      "TRAIN: \t Epoch: 2 \t Loss: 5.182957642881423\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.244788004061829\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.382540622044765\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.253989686877473\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.206858514929677\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.307739075153039\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.306426420205772\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.106451949672892\n",
      "TRAIN: \t Epoch: 2 \t Loss: 6.211268876280005\n",
      "TRAIN: \t Epoch: 2 \t Loss: 4.882118293279507\n",
      "TRAIN: \t Epoch: 2 \t Loss: 4.971682955666997\n",
      "TRAIN: \t Epoch: 3 \t Loss: 3.3815487470873884\n",
      "TRAIN: \t Epoch: 3 \t Loss: 3.3908267668130683\n",
      "TRAIN: \t Epoch: 3 \t Loss: 4.245644432990251\n",
      "TRAIN: \t Epoch: 3 \t Loss: 4.233825590140178\n",
      "TRAIN: \t Epoch: 3 \t Loss: 4.247237893645679\n",
      "TRAIN: \t Epoch: 3 \t Loss: 4.174854639067558\n",
      "TRAIN: \t Epoch: 3 \t Loss: 4.613546521261072\n",
      "TRAIN: \t Epoch: 3 \t Loss: 4.634819806612558\n",
      "TRAIN: \t Epoch: 3 \t Loss: 3.006869339077412\n",
      "TRAIN: \t Epoch: 3 \t Loss: 3.0831733151587373\n",
      "TRAIN: \t Epoch: 3 \t Loss: 3.685150603280316\n",
      "TRAIN: \t Epoch: 3 \t Loss: 3.5950371276535145\n",
      "TRAIN: \t Epoch: 3 \t Loss: 5.866990572453088\n",
      "TRAIN: \t Epoch: 3 \t Loss: 5.7942386171876\n",
      "TRAIN: \t Epoch: 3 \t Loss: 4.918087083863006\n",
      "TRAIN: \t Epoch: 3 \t Loss: 5.1573255466010846\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.492956211495889\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.493142110973971\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.94115804597849\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.943074337634788\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.079859208635595\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.188812720634857\n",
      "TRAIN: \t Epoch: 3 \t Loss: 5.775725045207054\n",
      "TRAIN: \t Epoch: 3 \t Loss: 5.710062640816894\n",
      "TRAIN: \t Epoch: 3 \t Loss: 5.192658968063556\n",
      "TRAIN: \t Epoch: 3 \t Loss: 5.0990393492926795\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.193546894991937\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.290753542877878\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.226882820133285\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.186003994025216\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.2578142219113015\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.224060023161752\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.042641609758858\n",
      "TRAIN: \t Epoch: 3 \t Loss: 6.130549743288189\n",
      "TRAIN: \t Epoch: 3 \t Loss: 4.6921733259146725\n",
      "TRAIN: \t Epoch: 3 \t Loss: 4.823537310247252\n",
      "TRAIN: \t Epoch: 4 \t Loss: 3.7145963834706395\n",
      "TRAIN: \t Epoch: 4 \t Loss: 3.6026400601374244\n",
      "TRAIN: \t Epoch: 4 \t Loss: 4.48270186737353\n",
      "TRAIN: \t Epoch: 4 \t Loss: 4.545472464825437\n",
      "TRAIN: \t Epoch: 4 \t Loss: 4.562171126210368\n",
      "TRAIN: \t Epoch: 4 \t Loss: 4.481030641241749\n",
      "TRAIN: \t Epoch: 4 \t Loss: 4.866786107443839\n",
      "TRAIN: \t Epoch: 4 \t Loss: 4.732885958898069\n",
      "TRAIN: \t Epoch: 4 \t Loss: 3.194983174370201\n",
      "TRAIN: \t Epoch: 4 \t Loss: 3.2388530625184044\n",
      "TRAIN: \t Epoch: 4 \t Loss: 3.768383341033404\n",
      "TRAIN: \t Epoch: 4 \t Loss: 3.6654618106939023\n",
      "TRAIN: \t Epoch: 4 \t Loss: 5.89458318053507\n",
      "TRAIN: \t Epoch: 4 \t Loss: 5.869741115963359\n",
      "TRAIN: \t Epoch: 4 \t Loss: 4.985047266050683\n",
      "TRAIN: \t Epoch: 4 \t Loss: 5.100832628100948\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.528414823015845\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.709059200551611\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.92593515434436\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.847491247839351\n",
      "TRAIN: \t Epoch: 4 \t Loss: 5.99039213935635\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.0497598301009265\n",
      "TRAIN: \t Epoch: 4 \t Loss: 5.623121991306928\n",
      "TRAIN: \t Epoch: 4 \t Loss: 5.621735190768101\n",
      "TRAIN: \t Epoch: 4 \t Loss: 5.216777619690596\n",
      "TRAIN: \t Epoch: 4 \t Loss: 5.0436244930054315\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.122969784009655\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.16545015478965\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.175336020226334\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.124800999978275\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.1388007311700505\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.20458531881514\n",
      "TRAIN: \t Epoch: 4 \t Loss: 5.9951674188795145\n",
      "TRAIN: \t Epoch: 4 \t Loss: 6.07171716452856\n",
      "TRAIN: \t Epoch: 4 \t Loss: 4.681879658081239\n",
      "TRAIN: \t Epoch: 4 \t Loss: 4.754107908561227\n",
      "TRAIN: \t Epoch: 5 \t Loss: 3.2808577099054927\n",
      "TRAIN: \t Epoch: 5 \t Loss: 3.371451440405023\n",
      "TRAIN: \t Epoch: 5 \t Loss: 4.081121674255563\n",
      "TRAIN: \t Epoch: 5 \t Loss: 4.173488938904829\n",
      "TRAIN: \t Epoch: 5 \t Loss: 4.215261284581673\n",
      "TRAIN: \t Epoch: 5 \t Loss: 4.178672461247709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 5 \t Loss: 4.450963650178811\n",
      "TRAIN: \t Epoch: 5 \t Loss: 4.38573975198695\n",
      "TRAIN: \t Epoch: 5 \t Loss: 3.0135117736057486\n",
      "TRAIN: \t Epoch: 5 \t Loss: 2.9888636916535978\n",
      "TRAIN: \t Epoch: 5 \t Loss: 3.62889752974633\n",
      "TRAIN: \t Epoch: 5 \t Loss: 3.568586621103368\n",
      "TRAIN: \t Epoch: 5 \t Loss: 5.815552296984096\n",
      "TRAIN: \t Epoch: 5 \t Loss: 5.77977491531818\n",
      "TRAIN: \t Epoch: 5 \t Loss: 4.855262846489159\n",
      "TRAIN: \t Epoch: 5 \t Loss: 4.991170063598151\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.451738264617149\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.602131266500959\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.758210685268232\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.8096979494662\n",
      "TRAIN: \t Epoch: 5 \t Loss: 5.8915748724918675\n",
      "TRAIN: \t Epoch: 5 \t Loss: 5.96498075931627\n",
      "TRAIN: \t Epoch: 5 \t Loss: 5.671491249351885\n",
      "TRAIN: \t Epoch: 5 \t Loss: 5.697043493474173\n",
      "TRAIN: \t Epoch: 5 \t Loss: 5.019925443073145\n",
      "TRAIN: \t Epoch: 5 \t Loss: 4.87898635830234\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.103506753551817\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.234110542311567\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.179427765571972\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.128832587724123\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.222374124292488\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.215636729789803\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.003398657298081\n",
      "TRAIN: \t Epoch: 5 \t Loss: 6.051296317255733\n",
      "TRAIN: \t Epoch: 5 \t Loss: 4.620354304923475\n",
      "TRAIN: \t Epoch: 5 \t Loss: 4.698670746443147\n",
      "TRAIN: \t Epoch: 6 \t Loss: 3.4264765195678426\n",
      "TRAIN: \t Epoch: 6 \t Loss: 3.5155624819083444\n",
      "TRAIN: \t Epoch: 6 \t Loss: 4.14383856247584\n",
      "TRAIN: \t Epoch: 6 \t Loss: 4.2277948485589265\n",
      "TRAIN: \t Epoch: 6 \t Loss: 4.348375748948058\n",
      "TRAIN: \t Epoch: 6 \t Loss: 4.253499343444423\n",
      "TRAIN: \t Epoch: 6 \t Loss: 4.581071696863037\n",
      "TRAIN: \t Epoch: 6 \t Loss: 4.423921956978976\n",
      "TRAIN: \t Epoch: 6 \t Loss: 3.058583350322987\n",
      "TRAIN: \t Epoch: 6 \t Loss: 3.0593353004273554\n",
      "TRAIN: \t Epoch: 6 \t Loss: 3.639931114562972\n",
      "TRAIN: \t Epoch: 6 \t Loss: 3.517111684416518\n",
      "TRAIN: \t Epoch: 6 \t Loss: 5.793184372226284\n",
      "TRAIN: \t Epoch: 6 \t Loss: 5.845085350764274\n",
      "TRAIN: \t Epoch: 6 \t Loss: 4.9232425771516715\n",
      "TRAIN: \t Epoch: 6 \t Loss: 5.012284730845669\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.470183565867684\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.516865589303174\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.724324641260577\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.741056756466911\n",
      "TRAIN: \t Epoch: 6 \t Loss: 5.795946311447716\n",
      "TRAIN: \t Epoch: 6 \t Loss: 5.935844244056197\n",
      "TRAIN: \t Epoch: 6 \t Loss: 5.7947195134154414\n",
      "TRAIN: \t Epoch: 6 \t Loss: 5.75600503745557\n",
      "TRAIN: \t Epoch: 6 \t Loss: 5.012633932371132\n",
      "TRAIN: \t Epoch: 6 \t Loss: 4.891448610944211\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.006069488523915\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.043635019048843\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.1233183627306005\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.083365130907141\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.170048086786106\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.162687732714882\n",
      "TRAIN: \t Epoch: 6 \t Loss: 5.99889921895756\n",
      "TRAIN: \t Epoch: 6 \t Loss: 6.05816372600137\n",
      "TRAIN: \t Epoch: 6 \t Loss: 4.623801123732884\n",
      "TRAIN: \t Epoch: 6 \t Loss: 4.641815106629274\n",
      "TRAIN: \t Epoch: 7 \t Loss: 3.1196091467834473\n",
      "TRAIN: \t Epoch: 7 \t Loss: 3.346773648400338\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.267208626469346\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.1542373130080605\n",
      "TRAIN: \t Epoch: 7 \t Loss: 3.990238350076036\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.0792501260590415\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.522301391729341\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.41235789712865\n",
      "TRAIN: \t Epoch: 7 \t Loss: 2.7038212469391523\n",
      "TRAIN: \t Epoch: 7 \t Loss: 2.809493456218587\n",
      "TRAIN: \t Epoch: 7 \t Loss: 3.7890508535875864\n",
      "TRAIN: \t Epoch: 7 \t Loss: 3.423914118368491\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.530539441688632\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.582847810956645\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.714481921667379\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.868293852573033\n",
      "TRAIN: \t Epoch: 7 \t Loss: 6.42450825646871\n",
      "TRAIN: \t Epoch: 7 \t Loss: 6.567314953694263\n",
      "TRAIN: \t Epoch: 7 \t Loss: 6.64646947992175\n",
      "TRAIN: \t Epoch: 7 \t Loss: 6.676468316098728\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.819731393116044\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.757068623627569\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.576273236964323\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.62800016736431\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.936833220064397\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.722063153703961\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.971724893068843\n",
      "TRAIN: \t Epoch: 7 \t Loss: 6.041032478446451\n",
      "TRAIN: \t Epoch: 7 \t Loss: 6.006120144462634\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.945258972174567\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.913743819540394\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.9830794910160785\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.836286671518513\n",
      "TRAIN: \t Epoch: 7 \t Loss: 5.873997639399136\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.34780594633204\n",
      "TRAIN: \t Epoch: 7 \t Loss: 4.444240179189208\n",
      "TRAIN: \t Epoch: 8 \t Loss: 3.1056468848306156\n",
      "TRAIN: \t Epoch: 8 \t Loss: 3.5216485107707705\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.620294609862623\n",
      "TRAIN: \t Epoch: 8 \t Loss: 3.9683361220928655\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.167532294818171\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.214468768025643\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.653675569141399\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.580916040353008\n",
      "TRAIN: \t Epoch: 8 \t Loss: 2.9128553296784965\n",
      "TRAIN: \t Epoch: 8 \t Loss: 3.043505617816149\n",
      "TRAIN: \t Epoch: 8 \t Loss: 3.6013598612533655\n",
      "TRAIN: \t Epoch: 8 \t Loss: 3.507672778689869\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.641242964265933\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.593363424635452\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.594060632520764\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.853359388151294\n",
      "TRAIN: \t Epoch: 8 \t Loss: 6.294378508236635\n",
      "TRAIN: \t Epoch: 8 \t Loss: 6.3578957426429215\n",
      "TRAIN: \t Epoch: 8 \t Loss: 6.6055733856504135\n",
      "TRAIN: \t Epoch: 8 \t Loss: 6.736471745139228\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.774655791367839\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.7595581610014435\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.459845374874087\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.597212085899331\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.923606540884498\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.638630008346535\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.8183191283388185\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.966312846620305\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.99047224415812\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.9338625373016525\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.888142521504671\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.936736608371593\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.788296287238971\n",
      "TRAIN: \t Epoch: 8 \t Loss: 5.876056712033103\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.344264474927226\n",
      "TRAIN: \t Epoch: 8 \t Loss: 4.420526717424433\n",
      "TRAIN: \t Epoch: 9 \t Loss: 2.9087435038302\n",
      "TRAIN: \t Epoch: 9 \t Loss: 3.1254968650802297\n",
      "TRAIN: \t Epoch: 9 \t Loss: 4.0385709934739475\n",
      "TRAIN: \t Epoch: 9 \t Loss: 3.9532625726571635\n",
      "TRAIN: \t Epoch: 9 \t Loss: 3.741029270490433\n",
      "TRAIN: \t Epoch: 9 \t Loss: 3.755213815307428\n",
      "TRAIN: \t Epoch: 9 \t Loss: 4.220740105945155\n",
      "TRAIN: \t Epoch: 9 \t Loss: 4.136529083557618\n",
      "TRAIN: \t Epoch: 9 \t Loss: 2.4866225871600887\n",
      "TRAIN: \t Epoch: 9 \t Loss: 2.5825544292619367\n",
      "TRAIN: \t Epoch: 9 \t Loss: 3.378312620176599\n",
      "TRAIN: \t Epoch: 9 \t Loss: 3.2143765037037446\n",
      "TRAIN: \t Epoch: 9 \t Loss: 5.275038892478221\n",
      "TRAIN: \t Epoch: 9 \t Loss: 5.328452592491806\n",
      "TRAIN: \t Epoch: 9 \t Loss: 4.295028856383267\n",
      "TRAIN: \t Epoch: 9 \t Loss: 4.484897311277173\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.07381439001112\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.159427090108114\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.439882659955236\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.576403218634368\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.261218046278493\n",
      "TRAIN: \t Epoch: 9 \t Loss: 10.454416690218823\n",
      "TRAIN: \t Epoch: 9 \t Loss: 7.908952778215914\n",
      "TRAIN: \t Epoch: 9 \t Loss: 7.957379422924515\n",
      "TRAIN: \t Epoch: 9 \t Loss: 10.185878531578386\n",
      "TRAIN: \t Epoch: 9 \t Loss: 10.029728635455694\n",
      "TRAIN: \t Epoch: 9 \t Loss: 7.74369311118446\n",
      "TRAIN: \t Epoch: 9 \t Loss: 7.320855156031964\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.290657430170817\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.335738390448458\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.478956823435343\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.5321781813083195\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.33214686585357\n",
      "TRAIN: \t Epoch: 9 \t Loss: 6.325811038377233\n",
      "TRAIN: \t Epoch: 9 \t Loss: 5.695544599592117\n",
      "TRAIN: \t Epoch: 9 \t Loss: 5.628356725980667\n",
      "TRAIN: \t Epoch: 10 \t Loss: 4.716030107553941\n",
      "TRAIN: \t Epoch: 10 \t Loss: 4.616120978602001\n",
      "TRAIN: \t Epoch: 10 \t Loss: 5.1118083132058745\n",
      "TRAIN: \t Epoch: 10 \t Loss: 5.075470355678717\n",
      "TRAIN: \t Epoch: 10 \t Loss: 4.947293389318591\n",
      "TRAIN: \t Epoch: 10 \t Loss: 4.865556181741566\n",
      "TRAIN: \t Epoch: 10 \t Loss: 5.2803433321056605\n",
      "TRAIN: \t Epoch: 10 \t Loss: 5.151640752975549\n",
      "TRAIN: \t Epoch: 10 \t Loss: 3.743176797476502\n",
      "TRAIN: \t Epoch: 10 \t Loss: 3.7512981552000273\n",
      "TRAIN: \t Epoch: 10 \t Loss: 4.11877259454326\n",
      "TRAIN: \t Epoch: 10 \t Loss: 3.9637503387792057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 10 \t Loss: 6.025657094453281\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.059471954771451\n",
      "TRAIN: \t Epoch: 10 \t Loss: 5.366661489399132\n",
      "TRAIN: \t Epoch: 10 \t Loss: 5.561970897041175\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.847239995719478\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.810365411506763\n",
      "TRAIN: \t Epoch: 10 \t Loss: 7.071890601807789\n",
      "TRAIN: \t Epoch: 10 \t Loss: 7.125538462171187\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.480687853030211\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.584172194282559\n",
      "TRAIN: \t Epoch: 10 \t Loss: 5.8737859027813055\n",
      "TRAIN: \t Epoch: 10 \t Loss: 5.83104589072857\n",
      "TRAIN: \t Epoch: 10 \t Loss: 5.527515365959557\n",
      "TRAIN: \t Epoch: 10 \t Loss: 5.402510939139146\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.203285663856521\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.285144580030246\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.341968915789712\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.204831037416037\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.264779131282818\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.292674300515696\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.087111231346379\n",
      "TRAIN: \t Epoch: 10 \t Loss: 6.123545481920674\n",
      "TRAIN: \t Epoch: 10 \t Loss: 4.955906173176181\n",
      "TRAIN: \t Epoch: 10 \t Loss: 4.995483319959652\n",
      "TRAIN: \t Epoch: 11 \t Loss: 3.6505123943602302\n",
      "TRAIN: \t Epoch: 11 \t Loss: 3.555559051354101\n",
      "TRAIN: \t Epoch: 11 \t Loss: 4.3475258237540295\n",
      "TRAIN: \t Epoch: 11 \t Loss: 4.260075921784516\n",
      "TRAIN: \t Epoch: 11 \t Loss: 4.342770807420355\n",
      "TRAIN: \t Epoch: 11 \t Loss: 4.3410911837565305\n",
      "TRAIN: \t Epoch: 11 \t Loss: 4.69087804993463\n",
      "TRAIN: \t Epoch: 11 \t Loss: 4.639519658291248\n",
      "TRAIN: \t Epoch: 11 \t Loss: 3.0258106474789828\n",
      "TRAIN: \t Epoch: 11 \t Loss: 3.091025993382859\n",
      "TRAIN: \t Epoch: 11 \t Loss: 3.488326672031053\n",
      "TRAIN: \t Epoch: 11 \t Loss: 3.5104469839983516\n",
      "TRAIN: \t Epoch: 11 \t Loss: 5.751236518003631\n",
      "TRAIN: \t Epoch: 11 \t Loss: 5.791097826711284\n",
      "TRAIN: \t Epoch: 11 \t Loss: 4.919029699287923\n",
      "TRAIN: \t Epoch: 11 \t Loss: 5.167267216541097\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.515864212868641\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.545034802752518\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.862877255460877\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.882938299177681\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.245958540021684\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.2353005221379\n",
      "TRAIN: \t Epoch: 11 \t Loss: 5.517452176076317\n",
      "TRAIN: \t Epoch: 11 \t Loss: 5.543780784009198\n",
      "TRAIN: \t Epoch: 11 \t Loss: 5.274555634731156\n",
      "TRAIN: \t Epoch: 11 \t Loss: 5.123967479400804\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.019746836016752\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.046839582252815\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.10277303599562\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.050369733760873\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.138459690793506\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.1373046757096965\n",
      "TRAIN: \t Epoch: 11 \t Loss: 5.953296378276919\n",
      "TRAIN: \t Epoch: 11 \t Loss: 6.00342620609277\n",
      "TRAIN: \t Epoch: 11 \t Loss: 4.795260515298844\n",
      "TRAIN: \t Epoch: 11 \t Loss: 4.792459718874719\n",
      "TRAIN: \t Epoch: 12 \t Loss: 3.3438140942195123\n",
      "TRAIN: \t Epoch: 12 \t Loss: 3.261410118719108\n",
      "TRAIN: \t Epoch: 12 \t Loss: 4.134564032247704\n",
      "TRAIN: \t Epoch: 12 \t Loss: 4.118384448121365\n",
      "TRAIN: \t Epoch: 12 \t Loss: 4.167290422910538\n",
      "TRAIN: \t Epoch: 12 \t Loss: 4.125650015583036\n",
      "TRAIN: \t Epoch: 12 \t Loss: 4.570072820302742\n",
      "TRAIN: \t Epoch: 12 \t Loss: 4.511538513163332\n",
      "TRAIN: \t Epoch: 12 \t Loss: 2.82953080880029\n",
      "TRAIN: \t Epoch: 12 \t Loss: 2.931760647408275\n",
      "TRAIN: \t Epoch: 12 \t Loss: 3.469117294837302\n",
      "TRAIN: \t Epoch: 12 \t Loss: 3.376655951952106\n",
      "TRAIN: \t Epoch: 12 \t Loss: 5.646844817516556\n",
      "TRAIN: \t Epoch: 12 \t Loss: 5.647141046870095\n",
      "TRAIN: \t Epoch: 12 \t Loss: 4.7992241254563766\n",
      "TRAIN: \t Epoch: 12 \t Loss: 5.052572404451436\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.484983395297633\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.48835656830469\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.736177072980209\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.811418725841506\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.086175077467609\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.09109948887197\n",
      "TRAIN: \t Epoch: 12 \t Loss: 5.487363310603492\n",
      "TRAIN: \t Epoch: 12 \t Loss: 5.415460961677447\n",
      "TRAIN: \t Epoch: 12 \t Loss: 5.124891127475505\n",
      "TRAIN: \t Epoch: 12 \t Loss: 4.983876310303112\n",
      "TRAIN: \t Epoch: 12 \t Loss: 5.9281333989366845\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.001236527741327\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.033647599288345\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.0060054107775125\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.048385190544251\n",
      "TRAIN: \t Epoch: 12 \t Loss: 6.074652453309313\n",
      "TRAIN: \t Epoch: 12 \t Loss: 5.87760800025896\n",
      "TRAIN: \t Epoch: 12 \t Loss: 5.923294238469842\n",
      "TRAIN: \t Epoch: 12 \t Loss: 4.635668958454065\n",
      "TRAIN: \t Epoch: 12 \t Loss: 4.7037908355773\n",
      "TRAIN: \t Epoch: 13 \t Loss: 3.2109536267100047\n",
      "TRAIN: \t Epoch: 13 \t Loss: 3.1371948870647532\n",
      "TRAIN: \t Epoch: 13 \t Loss: 3.977625058296025\n",
      "TRAIN: \t Epoch: 13 \t Loss: 4.04066089110991\n",
      "TRAIN: \t Epoch: 13 \t Loss: 4.114520054479374\n",
      "TRAIN: \t Epoch: 13 \t Loss: 3.997669942671135\n",
      "TRAIN: \t Epoch: 13 \t Loss: 4.398153190897336\n",
      "TRAIN: \t Epoch: 13 \t Loss: 4.348885253280849\n",
      "TRAIN: \t Epoch: 13 \t Loss: 2.836306963988254\n",
      "TRAIN: \t Epoch: 13 \t Loss: 2.854653312733236\n",
      "TRAIN: \t Epoch: 13 \t Loss: 3.360323278727113\n",
      "TRAIN: \t Epoch: 13 \t Loss: 3.3450016157905167\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.477382555799097\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.601104901591606\n",
      "TRAIN: \t Epoch: 13 \t Loss: 4.753555686440842\n",
      "TRAIN: \t Epoch: 13 \t Loss: 4.961229027929789\n",
      "TRAIN: \t Epoch: 13 \t Loss: 6.434678215135419\n",
      "TRAIN: \t Epoch: 13 \t Loss: 6.467768204907476\n",
      "TRAIN: \t Epoch: 13 \t Loss: 6.6629329448948384\n",
      "TRAIN: \t Epoch: 13 \t Loss: 6.742587398454195\n",
      "TRAIN: \t Epoch: 13 \t Loss: 6.0462841610669\n",
      "TRAIN: \t Epoch: 13 \t Loss: 6.058699927911151\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.449426253018652\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.384596814964224\n",
      "TRAIN: \t Epoch: 13 \t Loss: 4.992753483499425\n",
      "TRAIN: \t Epoch: 13 \t Loss: 4.896450582744206\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.960459694386398\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.96053104120124\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.954975835156736\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.9529930606944585\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.9995367468785314\n",
      "TRAIN: \t Epoch: 13 \t Loss: 6.021213994512358\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.808012356558795\n",
      "TRAIN: \t Epoch: 13 \t Loss: 5.873226577060051\n",
      "TRAIN: \t Epoch: 13 \t Loss: 4.5576731112078415\n",
      "TRAIN: \t Epoch: 13 \t Loss: 4.601336482051832\n",
      "TRAIN: \t Epoch: 14 \t Loss: 3.153497082851473\n",
      "TRAIN: \t Epoch: 14 \t Loss: 3.074319476764783\n",
      "TRAIN: \t Epoch: 14 \t Loss: 3.910929495468498\n",
      "TRAIN: \t Epoch: 14 \t Loss: 3.992721119747066\n",
      "TRAIN: \t Epoch: 14 \t Loss: 4.04496592591419\n",
      "TRAIN: \t Epoch: 14 \t Loss: 3.942453609205702\n",
      "TRAIN: \t Epoch: 14 \t Loss: 4.380867840454941\n",
      "TRAIN: \t Epoch: 14 \t Loss: 4.266075072813035\n",
      "TRAIN: \t Epoch: 14 \t Loss: 2.7220666091198016\n",
      "TRAIN: \t Epoch: 14 \t Loss: 2.864170990284157\n",
      "TRAIN: \t Epoch: 14 \t Loss: 3.370860060320798\n",
      "TRAIN: \t Epoch: 14 \t Loss: 3.340351230505293\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.451930352622599\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.53101304666939\n",
      "TRAIN: \t Epoch: 14 \t Loss: 4.703918166519265\n",
      "TRAIN: \t Epoch: 14 \t Loss: 4.930080444589567\n",
      "TRAIN: \t Epoch: 14 \t Loss: 6.32535241549178\n",
      "TRAIN: \t Epoch: 14 \t Loss: 6.361356121831652\n",
      "TRAIN: \t Epoch: 14 \t Loss: 6.619292628029809\n",
      "TRAIN: \t Epoch: 14 \t Loss: 6.662395095902145\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.9096395463410945\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.88975508559022\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.443001841912961\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.389959504358771\n",
      "TRAIN: \t Epoch: 14 \t Loss: 4.967013911569343\n",
      "TRAIN: \t Epoch: 14 \t Loss: 4.847364853420014\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.816413473085885\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.852745433770602\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.989902447532029\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.950448918766163\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.951376801454719\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.923285675423553\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.76004254158312\n",
      "TRAIN: \t Epoch: 14 \t Loss: 5.8161984654828\n",
      "TRAIN: \t Epoch: 14 \t Loss: 4.51219533832098\n",
      "TRAIN: \t Epoch: 14 \t Loss: 4.594244384078197\n",
      "TRAIN: \t Epoch: 15 \t Loss: 3.0686497765643965\n",
      "TRAIN: \t Epoch: 15 \t Loss: 3.1620762179012183\n",
      "TRAIN: \t Epoch: 15 \t Loss: 3.8319998588633997\n",
      "TRAIN: \t Epoch: 15 \t Loss: 3.936930720932836\n",
      "TRAIN: \t Epoch: 15 \t Loss: 3.998603264893738\n",
      "TRAIN: \t Epoch: 15 \t Loss: 3.9157355123051376\n",
      "TRAIN: \t Epoch: 15 \t Loss: 4.3464320282160624\n",
      "TRAIN: \t Epoch: 15 \t Loss: 4.248791202398799\n",
      "TRAIN: \t Epoch: 15 \t Loss: 2.6349555718020623\n",
      "TRAIN: \t Epoch: 15 \t Loss: 2.8329032522007216\n",
      "TRAIN: \t Epoch: 15 \t Loss: 3.3824933066312783\n",
      "TRAIN: \t Epoch: 15 \t Loss: 3.2834441601732096\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.426247311318517\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.477951093241353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 15 \t Loss: 4.605060651560944\n",
      "TRAIN: \t Epoch: 15 \t Loss: 4.918653360112188\n",
      "TRAIN: \t Epoch: 15 \t Loss: 6.232705180632735\n",
      "TRAIN: \t Epoch: 15 \t Loss: 6.278124144560232\n",
      "TRAIN: \t Epoch: 15 \t Loss: 6.612066255875715\n",
      "TRAIN: \t Epoch: 15 \t Loss: 6.661280147866034\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.815610239306986\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.765322018808267\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.330027793122047\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.335722028275343\n",
      "TRAIN: \t Epoch: 15 \t Loss: 4.871195443174952\n",
      "TRAIN: \t Epoch: 15 \t Loss: 4.7385004223564025\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.764532502813357\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.7776985421521605\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.8840144194933615\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.881761738548135\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.904970460555941\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.9460857882467755\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.684225843994417\n",
      "TRAIN: \t Epoch: 15 \t Loss: 5.792804588399486\n",
      "TRAIN: \t Epoch: 15 \t Loss: 4.443758003305709\n",
      "TRAIN: \t Epoch: 15 \t Loss: 4.51939561810507\n",
      "TRAIN: \t Epoch: 16 \t Loss: 2.9570315917072305\n",
      "TRAIN: \t Epoch: 16 \t Loss: 3.1111122196816527\n",
      "TRAIN: \t Epoch: 16 \t Loss: 3.8210558050360883\n",
      "TRAIN: \t Epoch: 16 \t Loss: 3.8384413807873576\n",
      "TRAIN: \t Epoch: 16 \t Loss: 3.8492680590820365\n",
      "TRAIN: \t Epoch: 16 \t Loss: 3.8781272691837647\n",
      "TRAIN: \t Epoch: 16 \t Loss: 4.285048714919285\n",
      "TRAIN: \t Epoch: 16 \t Loss: 4.176420875061359\n",
      "TRAIN: \t Epoch: 16 \t Loss: 2.5892296765401994\n",
      "TRAIN: \t Epoch: 16 \t Loss: 2.6694569392858725\n",
      "TRAIN: \t Epoch: 16 \t Loss: 3.326648863113725\n",
      "TRAIN: \t Epoch: 16 \t Loss: 3.1782639638265295\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.314493299888162\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.413027151879438\n",
      "TRAIN: \t Epoch: 16 \t Loss: 4.5316566677939445\n",
      "TRAIN: \t Epoch: 16 \t Loss: 4.797354301466307\n",
      "TRAIN: \t Epoch: 16 \t Loss: 6.277299616738484\n",
      "TRAIN: \t Epoch: 16 \t Loss: 6.212312132726369\n",
      "TRAIN: \t Epoch: 16 \t Loss: 6.485567011313336\n",
      "TRAIN: \t Epoch: 16 \t Loss: 6.492501771720571\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.736428456726394\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.657928830413788\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.269276564743372\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.308994858379477\n",
      "TRAIN: \t Epoch: 16 \t Loss: 4.738585446006262\n",
      "TRAIN: \t Epoch: 16 \t Loss: 4.6473660064592295\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.707768430021675\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.767903076985495\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.795744376822062\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.800177023508837\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.8118904870463615\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.885741112729345\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.626347137305924\n",
      "TRAIN: \t Epoch: 16 \t Loss: 5.708904783792805\n",
      "TRAIN: \t Epoch: 16 \t Loss: 4.279627721628886\n",
      "TRAIN: \t Epoch: 16 \t Loss: 4.358612249396948\n",
      "TRAIN: \t Epoch: 17 \t Loss: 2.8765421903942525\n",
      "TRAIN: \t Epoch: 17 \t Loss: 3.361034799025653\n",
      "TRAIN: \t Epoch: 17 \t Loss: 6.255338363366956\n",
      "TRAIN: \t Epoch: 17 \t Loss: 3.9777900118666043\n",
      "TRAIN: \t Epoch: 17 \t Loss: 4.436991974958875\n",
      "TRAIN: \t Epoch: 17 \t Loss: 4.740888785863526\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.208338157559995\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.146010185621009\n",
      "TRAIN: \t Epoch: 17 \t Loss: 3.8588316154633064\n",
      "TRAIN: \t Epoch: 17 \t Loss: 3.806294816230738\n",
      "TRAIN: \t Epoch: 17 \t Loss: 4.027448028536331\n",
      "TRAIN: \t Epoch: 17 \t Loss: 4.055740752650502\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.895611730468234\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.771200124061001\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.168751885927006\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.29968991792757\n",
      "TRAIN: \t Epoch: 17 \t Loss: 6.622032174540209\n",
      "TRAIN: \t Epoch: 17 \t Loss: 6.553370734419237\n",
      "TRAIN: \t Epoch: 17 \t Loss: 6.836974944811674\n",
      "TRAIN: \t Epoch: 17 \t Loss: 6.862522960274752\n",
      "TRAIN: \t Epoch: 17 \t Loss: 6.3110242121019\n",
      "TRAIN: \t Epoch: 17 \t Loss: 6.317906098213826\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.734052613080765\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.580280167748718\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.289581870823474\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.118292494883368\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.987913922318289\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.971630018264596\n",
      "TRAIN: \t Epoch: 17 \t Loss: 6.175537960373129\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.972627715719541\n",
      "TRAIN: \t Epoch: 17 \t Loss: 6.139354786454509\n",
      "TRAIN: \t Epoch: 17 \t Loss: 6.088542753510044\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.863996665057338\n",
      "TRAIN: \t Epoch: 17 \t Loss: 5.872434565436653\n",
      "TRAIN: \t Epoch: 17 \t Loss: 4.706296553118927\n",
      "TRAIN: \t Epoch: 17 \t Loss: 4.812103606905962\n",
      "TRAIN: \t Epoch: 18 \t Loss: 3.374016969065841\n",
      "TRAIN: \t Epoch: 18 \t Loss: 3.423127703481462\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.0662312822732405\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.052888823901473\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.146416081888816\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.082900665074761\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.5010650318037735\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.4599630153013\n",
      "TRAIN: \t Epoch: 18 \t Loss: 2.9615552591468814\n",
      "TRAIN: \t Epoch: 18 \t Loss: 2.9633978789979314\n",
      "TRAIN: \t Epoch: 18 \t Loss: 3.430084215809734\n",
      "TRAIN: \t Epoch: 18 \t Loss: 3.37404943558405\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.417742217385733\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.491871792288995\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.657469627442847\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.856975806201036\n",
      "TRAIN: \t Epoch: 18 \t Loss: 6.299488673232569\n",
      "TRAIN: \t Epoch: 18 \t Loss: 6.291820465172759\n",
      "TRAIN: \t Epoch: 18 \t Loss: 6.542420790049837\n",
      "TRAIN: \t Epoch: 18 \t Loss: 6.559930633193223\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.814481520863181\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.81253185842704\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.31085827328814\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.266191946956468\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.8436371245437755\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.765318657494163\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.8434204870308495\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.860329122002373\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.894719424387427\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.776071563969339\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.916593735982891\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.984137353428025\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.716250943050291\n",
      "TRAIN: \t Epoch: 18 \t Loss: 5.781529635442317\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.405466710404078\n",
      "TRAIN: \t Epoch: 18 \t Loss: 4.4583948745013\n",
      "TRAIN: \t Epoch: 19 \t Loss: 2.945731427597619\n",
      "TRAIN: \t Epoch: 19 \t Loss: 3.0218065144106787\n",
      "TRAIN: \t Epoch: 19 \t Loss: 3.801960978864587\n",
      "TRAIN: \t Epoch: 19 \t Loss: 3.8094712494028573\n",
      "TRAIN: \t Epoch: 19 \t Loss: 3.8818675780726233\n",
      "TRAIN: \t Epoch: 19 \t Loss: 3.829640164957793\n",
      "TRAIN: \t Epoch: 19 \t Loss: 4.207624981015501\n",
      "TRAIN: \t Epoch: 19 \t Loss: 4.1464839989605204\n",
      "TRAIN: \t Epoch: 19 \t Loss: 2.6167192105789945\n",
      "TRAIN: \t Epoch: 19 \t Loss: 2.7076673490445984\n",
      "TRAIN: \t Epoch: 19 \t Loss: 3.357479313086569\n",
      "TRAIN: \t Epoch: 19 \t Loss: 3.277178064861283\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.340030358980529\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.362852811287313\n",
      "TRAIN: \t Epoch: 19 \t Loss: 4.432814027463737\n",
      "TRAIN: \t Epoch: 19 \t Loss: 4.703741904582678\n",
      "TRAIN: \t Epoch: 19 \t Loss: 6.165876543975589\n",
      "TRAIN: \t Epoch: 19 \t Loss: 6.072067519125994\n",
      "TRAIN: \t Epoch: 19 \t Loss: 6.443963237217959\n",
      "TRAIN: \t Epoch: 19 \t Loss: 6.51898195495612\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.646722915362457\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.646944270284871\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.180431666817512\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.196302422710741\n",
      "TRAIN: \t Epoch: 19 \t Loss: 4.684770774066363\n",
      "TRAIN: \t Epoch: 19 \t Loss: 4.577557443531088\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.729256647509953\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.780261792090664\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.8018969615340765\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.703061206380414\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.7522925905873725\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.870236559692617\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.5983509144913235\n",
      "TRAIN: \t Epoch: 19 \t Loss: 5.655189728040533\n",
      "TRAIN: \t Epoch: 19 \t Loss: 4.285968335243377\n",
      "TRAIN: \t Epoch: 19 \t Loss: 4.270506757224982\n",
      "TRAIN: \t Epoch: 20 \t Loss: 2.897813260122457\n",
      "TRAIN: \t Epoch: 20 \t Loss: 2.994959876663264\n",
      "TRAIN: \t Epoch: 20 \t Loss: 3.7397587534934127\n",
      "TRAIN: \t Epoch: 20 \t Loss: 3.7341731976821197\n",
      "TRAIN: \t Epoch: 20 \t Loss: 3.7473119178890806\n",
      "TRAIN: \t Epoch: 20 \t Loss: 3.770428957232332\n",
      "TRAIN: \t Epoch: 20 \t Loss: 4.135531540605015\n",
      "TRAIN: \t Epoch: 20 \t Loss: 3.994369218430689\n",
      "TRAIN: \t Epoch: 20 \t Loss: 2.4813263742293814\n",
      "TRAIN: \t Epoch: 20 \t Loss: 2.6086280306396032\n",
      "TRAIN: \t Epoch: 20 \t Loss: 3.3831548933864495\n",
      "TRAIN: \t Epoch: 20 \t Loss: 3.179065571794734\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.173516298111804\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.279624572836739\n",
      "TRAIN: \t Epoch: 20 \t Loss: 4.3392075907505845\n",
      "TRAIN: \t Epoch: 20 \t Loss: 4.549221300733439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 20 \t Loss: 6.1497580522286714\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.989313982939369\n",
      "TRAIN: \t Epoch: 20 \t Loss: 6.333107081110839\n",
      "TRAIN: \t Epoch: 20 \t Loss: 6.365083409739265\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.501738803282141\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.525911709599141\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.182468619811947\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.232794433629017\n",
      "TRAIN: \t Epoch: 20 \t Loss: 4.5486568262847005\n",
      "TRAIN: \t Epoch: 20 \t Loss: 4.401057350290429\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.6164667336213725\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.647139734747079\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.706442154217402\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.665852471830067\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.607528989310623\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.721747272032137\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.473632019521112\n",
      "TRAIN: \t Epoch: 20 \t Loss: 5.539806783515711\n",
      "TRAIN: \t Epoch: 20 \t Loss: 4.081743011672749\n",
      "TRAIN: \t Epoch: 20 \t Loss: 4.205417625129765\n",
      "TRAIN: \t Epoch: 21 \t Loss: 3.0620625280145033\n",
      "TRAIN: \t Epoch: 21 \t Loss: 3.935606060199022\n",
      "TRAIN: \t Epoch: 21 \t Loss: 3.7343644789429673\n",
      "TRAIN: \t Epoch: 21 \t Loss: 3.9932119722115784\n",
      "TRAIN: \t Epoch: 21 \t Loss: 4.16757470724292\n",
      "TRAIN: \t Epoch: 21 \t Loss: 4.282720316694673\n",
      "TRAIN: \t Epoch: 21 \t Loss: 4.660711562566955\n",
      "TRAIN: \t Epoch: 21 \t Loss: 4.559913978763601\n",
      "TRAIN: \t Epoch: 21 \t Loss: 3.05628437616436\n",
      "TRAIN: \t Epoch: 21 \t Loss: 3.0466929751429745\n",
      "TRAIN: \t Epoch: 21 \t Loss: 3.4543737142619695\n",
      "TRAIN: \t Epoch: 21 \t Loss: 3.370332417987077\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.598730346034824\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.605738057042364\n",
      "TRAIN: \t Epoch: 21 \t Loss: 4.839247537495792\n",
      "TRAIN: \t Epoch: 21 \t Loss: 4.928378179496898\n",
      "TRAIN: \t Epoch: 21 \t Loss: 6.262822243650646\n",
      "TRAIN: \t Epoch: 21 \t Loss: 6.22167908034983\n",
      "TRAIN: \t Epoch: 21 \t Loss: 6.5935324134634765\n",
      "TRAIN: \t Epoch: 21 \t Loss: 6.6348421341996975\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.943625608629148\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.989255899321138\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.304900238443688\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.249727615528053\n",
      "TRAIN: \t Epoch: 21 \t Loss: 4.842106957082745\n",
      "TRAIN: \t Epoch: 21 \t Loss: 4.776433003416061\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.72282717272588\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.804307004701198\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.921399203458082\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.83566200363259\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.840209534212876\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.896649861100185\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.627623026321557\n",
      "TRAIN: \t Epoch: 21 \t Loss: 5.740956132797268\n",
      "TRAIN: \t Epoch: 21 \t Loss: 4.296176525639073\n",
      "TRAIN: \t Epoch: 21 \t Loss: 4.507420802346998\n",
      "ADE: 100.3768204579097  FDE: 25.249356576196252\n",
      "TRAIN: \t Epoch: 22 \t Loss: 2.9310312813142754\n",
      "TRAIN: \t Epoch: 22 \t Loss: 2.995176788470303\n",
      "TRAIN: \t Epoch: 22 \t Loss: 3.726445165970558\n",
      "TRAIN: \t Epoch: 22 \t Loss: 3.807116222703568\n",
      "TRAIN: \t Epoch: 22 \t Loss: 3.758418238200682\n",
      "TRAIN: \t Epoch: 22 \t Loss: 3.8052004144533265\n",
      "TRAIN: \t Epoch: 22 \t Loss: 4.176419808350438\n",
      "TRAIN: \t Epoch: 22 \t Loss: 4.099267316476244\n",
      "TRAIN: \t Epoch: 22 \t Loss: 2.542659042120161\n",
      "TRAIN: \t Epoch: 22 \t Loss: 2.632183242665028\n",
      "TRAIN: \t Epoch: 22 \t Loss: 3.2578116299971516\n",
      "TRAIN: \t Epoch: 22 \t Loss: 3.1872601985323947\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.186416816459318\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.304914022901915\n",
      "TRAIN: \t Epoch: 22 \t Loss: 4.355429960029861\n",
      "TRAIN: \t Epoch: 22 \t Loss: 4.600664846087532\n",
      "TRAIN: \t Epoch: 22 \t Loss: 6.192360285591865\n",
      "TRAIN: \t Epoch: 22 \t Loss: 6.008909479166277\n",
      "TRAIN: \t Epoch: 22 \t Loss: 6.439247948695918\n",
      "TRAIN: \t Epoch: 22 \t Loss: 6.496483859586078\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.672942756775287\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.663164336271031\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.1884282884332595\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.140499236791484\n",
      "TRAIN: \t Epoch: 22 \t Loss: 4.6861146812860355\n",
      "TRAIN: \t Epoch: 22 \t Loss: 4.57544978058644\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.668095750155925\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.7653724151708134\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.7868836337541945\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.718909584598174\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.6599868017167525\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.8846816396571775\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.584482225588251\n",
      "TRAIN: \t Epoch: 22 \t Loss: 5.763466410842446\n",
      "TRAIN: \t Epoch: 22 \t Loss: 4.206489942472048\n",
      "TRAIN: \t Epoch: 22 \t Loss: 4.3200453521334685\n",
      "ADE: 97.6778823914901  FDE: 25.631818717194662\n",
      "TRAIN: \t Epoch: 23 \t Loss: 2.855153548254032\n",
      "TRAIN: \t Epoch: 23 \t Loss: 3.0076224590636444\n",
      "TRAIN: \t Epoch: 23 \t Loss: 3.654219729276894\n",
      "TRAIN: \t Epoch: 23 \t Loss: 3.6600387790276865\n",
      "TRAIN: \t Epoch: 23 \t Loss: 3.6507745121071586\n",
      "TRAIN: \t Epoch: 23 \t Loss: 3.7259496629054163\n",
      "TRAIN: \t Epoch: 23 \t Loss: 4.086981747696266\n",
      "TRAIN: \t Epoch: 23 \t Loss: 4.0542594209353595\n",
      "TRAIN: \t Epoch: 23 \t Loss: 2.463534259584555\n",
      "TRAIN: \t Epoch: 23 \t Loss: 2.6036419392144454\n",
      "TRAIN: \t Epoch: 23 \t Loss: 3.3647968533269097\n",
      "TRAIN: \t Epoch: 23 \t Loss: 3.2061872888410377\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.134181715985932\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.207270951709879\n",
      "TRAIN: \t Epoch: 23 \t Loss: 4.237689587918514\n",
      "TRAIN: \t Epoch: 23 \t Loss: 4.497561023708704\n",
      "TRAIN: \t Epoch: 23 \t Loss: 6.00995780532129\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.988883507875794\n",
      "TRAIN: \t Epoch: 23 \t Loss: 6.3247271583017834\n",
      "TRAIN: \t Epoch: 23 \t Loss: 6.370153197011879\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.424520783486959\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.4520700026319036\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.099595339692891\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.101723729150905\n",
      "TRAIN: \t Epoch: 23 \t Loss: 4.445752599917402\n",
      "TRAIN: \t Epoch: 23 \t Loss: 4.380559297721735\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.60757336915475\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.623530822999292\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.66045943560069\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.632272290618057\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.600468463753847\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.715515583645097\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.5261001595428505\n",
      "TRAIN: \t Epoch: 23 \t Loss: 5.582211086983698\n",
      "TRAIN: \t Epoch: 23 \t Loss: 4.047471865801027\n",
      "TRAIN: \t Epoch: 23 \t Loss: 4.211501612863433\n",
      "ADE: 97.94873601469124  FDE: 25.819289787886362\n",
      "TRAIN: \t Epoch: 24 \t Loss: 2.878688875943092\n",
      "TRAIN: \t Epoch: 24 \t Loss: 3.5051251451266277\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.424579170323612\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.057279098243547\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.040196952683815\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.108834457494891\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.660680785496059\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.640283289598467\n",
      "TRAIN: \t Epoch: 24 \t Loss: 3.220498081268899\n",
      "TRAIN: \t Epoch: 24 \t Loss: 3.1801776085042825\n",
      "TRAIN: \t Epoch: 24 \t Loss: 3.534019137192844\n",
      "TRAIN: \t Epoch: 24 \t Loss: 3.4967514631857437\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.510983529375814\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.529242318219673\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.897573564869995\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.001059755887858\n",
      "TRAIN: \t Epoch: 24 \t Loss: 6.352611968152953\n",
      "TRAIN: \t Epoch: 24 \t Loss: 6.260402711601834\n",
      "TRAIN: \t Epoch: 24 \t Loss: 6.572756038071169\n",
      "TRAIN: \t Epoch: 24 \t Loss: 6.602927680881988\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.900805401643208\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.935688781686274\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.288800660848025\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.194182260127295\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.755862300400738\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.6446910342202825\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.663204464466249\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.651447008543615\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.824950886409861\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.759346759151273\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.728760608988886\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.788999534206144\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.554419544822884\n",
      "TRAIN: \t Epoch: 24 \t Loss: 5.62308183174426\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.220132878109385\n",
      "TRAIN: \t Epoch: 24 \t Loss: 4.3359670702531465\n",
      "ADE: 99.00999733977228  FDE: 24.474465423544398\n",
      "TRAIN: \t Epoch: 25 \t Loss: 2.8586390495201934\n",
      "TRAIN: \t Epoch: 25 \t Loss: 2.993370847081943\n",
      "TRAIN: \t Epoch: 25 \t Loss: 3.582514885862605\n",
      "TRAIN: \t Epoch: 25 \t Loss: 3.626477471084672\n",
      "TRAIN: \t Epoch: 25 \t Loss: 3.6710276811606954\n",
      "TRAIN: \t Epoch: 25 \t Loss: 3.7012906658040885\n",
      "TRAIN: \t Epoch: 25 \t Loss: 4.043713511410538\n",
      "TRAIN: \t Epoch: 25 \t Loss: 3.9989401308267976\n",
      "TRAIN: \t Epoch: 25 \t Loss: 2.4503192079818614\n",
      "TRAIN: \t Epoch: 25 \t Loss: 2.50677052760832\n",
      "TRAIN: \t Epoch: 25 \t Loss: 3.2442334686655903\n",
      "TRAIN: \t Epoch: 25 \t Loss: 3.106565479448579\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.066811416539117\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.154328622972567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 25 \t Loss: 4.1843130890022655\n",
      "TRAIN: \t Epoch: 25 \t Loss: 4.52283762722356\n",
      "TRAIN: \t Epoch: 25 \t Loss: 6.000041527037242\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.932856744757314\n",
      "TRAIN: \t Epoch: 25 \t Loss: 6.276429293751143\n",
      "TRAIN: \t Epoch: 25 \t Loss: 6.342422890789225\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.425722354462498\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.427130073006158\n",
      "TRAIN: \t Epoch: 25 \t Loss: 4.991492010913867\n",
      "TRAIN: \t Epoch: 25 \t Loss: 4.9804601717152455\n",
      "TRAIN: \t Epoch: 25 \t Loss: 4.3692712140211185\n",
      "TRAIN: \t Epoch: 25 \t Loss: 4.290654366558588\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.444595120269982\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.495594290908223\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.592469589038775\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.539950136892133\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.48068801319345\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.64614658766731\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.387433078366934\n",
      "TRAIN: \t Epoch: 25 \t Loss: 5.481594172364047\n",
      "TRAIN: \t Epoch: 25 \t Loss: 3.8945280049315096\n",
      "TRAIN: \t Epoch: 25 \t Loss: 4.0526741008372085\n",
      "ADE: 98.34515442120872  FDE: 23.603018554619915\n",
      "TRAIN: \t Epoch: 26 \t Loss: 3.008366609167304\n",
      "TRAIN: \t Epoch: 26 \t Loss: 3.1449059810202535\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.308263265100574\n",
      "TRAIN: \t Epoch: 26 \t Loss: 4.448082569050931\n",
      "TRAIN: \t Epoch: 26 \t Loss: 4.6586012064530005\n",
      "TRAIN: \t Epoch: 26 \t Loss: 4.761753693555396\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.319857105711066\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.2008935903185645\n",
      "TRAIN: \t Epoch: 26 \t Loss: 4.085785716776671\n",
      "TRAIN: \t Epoch: 26 \t Loss: 4.272734942572612\n",
      "TRAIN: \t Epoch: 26 \t Loss: 4.430041110335568\n",
      "TRAIN: \t Epoch: 26 \t Loss: 4.395066882966147\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.993565736358237\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.955778658955346\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.456781018777026\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.518135327853215\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.764368585163884\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.665816344646839\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.904234952777491\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.947031645978918\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.470425580871572\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.461927933311089\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.75497256101521\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.706465333600233\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.538016800016785\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.46883018807343\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.020105088467496\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.058278084063744\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.118438316816059\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.071895993985049\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.193567091803352\n",
      "TRAIN: \t Epoch: 26 \t Loss: 6.2717892947306835\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.862273224214071\n",
      "TRAIN: \t Epoch: 26 \t Loss: 5.914458529992486\n",
      "TRAIN: \t Epoch: 26 \t Loss: 4.956754404842184\n",
      "TRAIN: \t Epoch: 26 \t Loss: 4.877122715679392\n",
      "ADE: 96.80005618477139  FDE: 27.046081487844297\n",
      "TRAIN: \t Epoch: 27 \t Loss: 3.764441345269019\n",
      "TRAIN: \t Epoch: 27 \t Loss: 3.760726754902287\n",
      "TRAIN: \t Epoch: 27 \t Loss: 4.444863566752849\n",
      "TRAIN: \t Epoch: 27 \t Loss: 4.371637357773935\n",
      "TRAIN: \t Epoch: 27 \t Loss: 4.325237430405419\n",
      "TRAIN: \t Epoch: 27 \t Loss: 4.409939989105427\n",
      "TRAIN: \t Epoch: 27 \t Loss: 4.723513070091338\n",
      "TRAIN: \t Epoch: 27 \t Loss: 4.702992032755082\n",
      "TRAIN: \t Epoch: 27 \t Loss: 3.2315456233708852\n",
      "TRAIN: \t Epoch: 27 \t Loss: 3.3554929917967256\n",
      "TRAIN: \t Epoch: 27 \t Loss: 3.625222442249985\n",
      "TRAIN: \t Epoch: 27 \t Loss: 3.6368849068489015\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.569627838168761\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.695788205990732\n",
      "TRAIN: \t Epoch: 27 \t Loss: 4.861292616319139\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.071831073319114\n",
      "TRAIN: \t Epoch: 27 \t Loss: 6.506405140020263\n",
      "TRAIN: \t Epoch: 27 \t Loss: 6.408488155209355\n",
      "TRAIN: \t Epoch: 27 \t Loss: 6.674184768110601\n",
      "TRAIN: \t Epoch: 27 \t Loss: 6.741050507598262\n",
      "TRAIN: \t Epoch: 27 \t Loss: 6.103311293836591\n",
      "TRAIN: \t Epoch: 27 \t Loss: 6.096490231570308\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.4231389787865965\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.405642029002216\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.015855719973194\n",
      "TRAIN: \t Epoch: 27 \t Loss: 4.983178744008018\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.802148903694744\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.82703770130558\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.869784655042185\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.818078398507032\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.948464805096106\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.991344406949491\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.678791755465932\n",
      "TRAIN: \t Epoch: 27 \t Loss: 5.725869690149754\n",
      "TRAIN: \t Epoch: 27 \t Loss: 4.584834687488009\n",
      "TRAIN: \t Epoch: 27 \t Loss: 4.540881416231304\n",
      "ADE: 96.94002352528179  FDE: 24.888378151709773\n",
      "TRAIN: \t Epoch: 28 \t Loss: 3.3194343340487666\n",
      "TRAIN: \t Epoch: 28 \t Loss: 3.300686834993928\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.110656380951223\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.076088610988579\n",
      "TRAIN: \t Epoch: 28 \t Loss: 3.996149410098692\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.0632267585853805\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.4653799191823875\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.4264461438523695\n",
      "TRAIN: \t Epoch: 28 \t Loss: 2.8671369189453046\n",
      "TRAIN: \t Epoch: 28 \t Loss: 2.995064917936247\n",
      "TRAIN: \t Epoch: 28 \t Loss: 3.4521470727687302\n",
      "TRAIN: \t Epoch: 28 \t Loss: 3.402441528175739\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.329519672798044\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.453862669855745\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.632451041516257\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.889200973095604\n",
      "TRAIN: \t Epoch: 28 \t Loss: 6.348879652038613\n",
      "TRAIN: \t Epoch: 28 \t Loss: 6.254554510094495\n",
      "TRAIN: \t Epoch: 28 \t Loss: 6.519220023415917\n",
      "TRAIN: \t Epoch: 28 \t Loss: 6.587704166587304\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.9513076835124865\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.9315225939856475\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.337695882306133\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.270727454198145\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.936010885026181\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.885012735045814\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.6682140241667796\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.733000105656872\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.854056121120675\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.778805646626417\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.8980000079863375\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.888054031597875\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.571540759733596\n",
      "TRAIN: \t Epoch: 28 \t Loss: 5.667671350504289\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.4820483140424106\n",
      "TRAIN: \t Epoch: 28 \t Loss: 4.456613545327319\n",
      "ADE: 98.7937092664358  FDE: 24.125606130156886\n",
      "TRAIN: \t Epoch: 29 \t Loss: 3.0741443246161455\n",
      "TRAIN: \t Epoch: 29 \t Loss: 3.092327512137318\n",
      "TRAIN: \t Epoch: 29 \t Loss: 3.9365742074576557\n",
      "TRAIN: \t Epoch: 29 \t Loss: 3.9076682992235163\n",
      "TRAIN: \t Epoch: 29 \t Loss: 3.799465127313065\n",
      "TRAIN: \t Epoch: 29 \t Loss: 3.8837731670550477\n",
      "TRAIN: \t Epoch: 29 \t Loss: 4.291018441883638\n",
      "TRAIN: \t Epoch: 29 \t Loss: 4.235567421186583\n",
      "TRAIN: \t Epoch: 29 \t Loss: 2.637834591411522\n",
      "TRAIN: \t Epoch: 29 \t Loss: 2.752095101763635\n",
      "TRAIN: \t Epoch: 29 \t Loss: 3.30637152734582\n",
      "TRAIN: \t Epoch: 29 \t Loss: 3.2758426713866116\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.217760002808248\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.38354250969685\n",
      "TRAIN: \t Epoch: 29 \t Loss: 4.4973777506455015\n",
      "TRAIN: \t Epoch: 29 \t Loss: 4.704419292320352\n",
      "TRAIN: \t Epoch: 29 \t Loss: 6.268171898857678\n",
      "TRAIN: \t Epoch: 29 \t Loss: 6.166889515120566\n",
      "TRAIN: \t Epoch: 29 \t Loss: 6.49349946101054\n",
      "TRAIN: \t Epoch: 29 \t Loss: 6.499780456320638\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.813071869425096\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.833780189214195\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.328689268548592\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.219728812317735\n",
      "TRAIN: \t Epoch: 29 \t Loss: 4.8036187004632405\n",
      "TRAIN: \t Epoch: 29 \t Loss: 4.714580938490023\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.636144999457995\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.659518580456923\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.772296978631329\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.7091738810249675\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.769742900593451\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.7634725745274205\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.493238496114005\n",
      "TRAIN: \t Epoch: 29 \t Loss: 5.6128134284817985\n",
      "TRAIN: \t Epoch: 29 \t Loss: 4.359168956692087\n",
      "TRAIN: \t Epoch: 29 \t Loss: 4.326990304095055\n",
      "ADE: 96.1824629546762  FDE: 25.191822447163375\n",
      "TRAIN: \t Epoch: 30 \t Loss: 2.9153452669496818\n",
      "TRAIN: \t Epoch: 30 \t Loss: 2.97238706257089\n",
      "TRAIN: \t Epoch: 30 \t Loss: 3.749875536327056\n",
      "TRAIN: \t Epoch: 30 \t Loss: 3.7056903429290293\n",
      "TRAIN: \t Epoch: 30 \t Loss: 3.7405218709738444\n",
      "TRAIN: \t Epoch: 30 \t Loss: 3.762411182028782\n",
      "TRAIN: \t Epoch: 30 \t Loss: 4.147650119789494\n",
      "TRAIN: \t Epoch: 30 \t Loss: 4.093948078440785\n",
      "TRAIN: \t Epoch: 30 \t Loss: 2.447529968430825\n",
      "TRAIN: \t Epoch: 30 \t Loss: 2.612151892761547\n",
      "TRAIN: \t Epoch: 30 \t Loss: 3.2654675993025433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 30 \t Loss: 3.149829672558234\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.159700441988932\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.234983689573426\n",
      "TRAIN: \t Epoch: 30 \t Loss: 4.390758652146978\n",
      "TRAIN: \t Epoch: 30 \t Loss: 4.623517492115174\n",
      "TRAIN: \t Epoch: 30 \t Loss: 6.194670207728921\n",
      "TRAIN: \t Epoch: 30 \t Loss: 6.11073432374198\n",
      "TRAIN: \t Epoch: 30 \t Loss: 6.358436872407319\n",
      "TRAIN: \t Epoch: 30 \t Loss: 6.402118812869565\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.650915582984951\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.61074149326487\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.219423390756178\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.2090973618259016\n",
      "TRAIN: \t Epoch: 30 \t Loss: 4.630152384669593\n",
      "TRAIN: \t Epoch: 30 \t Loss: 4.538924885767205\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.511568155212281\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.544045194564953\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.710193502310157\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.64141716240268\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.669759100979808\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.690679167337635\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.392187097452464\n",
      "TRAIN: \t Epoch: 30 \t Loss: 5.54776387427557\n",
      "TRAIN: \t Epoch: 30 \t Loss: 4.237449410887181\n",
      "TRAIN: \t Epoch: 30 \t Loss: 4.273957398381076\n",
      "ADE: 96.93206552526814  FDE: 24.44783990895289\n",
      "TRAIN: \t Epoch: 31 \t Loss: 2.842217204531923\n",
      "TRAIN: \t Epoch: 31 \t Loss: 2.9787494380896287\n",
      "TRAIN: \t Epoch: 31 \t Loss: 3.803251136508573\n",
      "TRAIN: \t Epoch: 31 \t Loss: 3.725209128746338\n",
      "TRAIN: \t Epoch: 31 \t Loss: 3.737098018565071\n",
      "TRAIN: \t Epoch: 31 \t Loss: 3.7062863939363604\n",
      "TRAIN: \t Epoch: 31 \t Loss: 4.11825409650969\n",
      "TRAIN: \t Epoch: 31 \t Loss: 4.019691101649483\n",
      "TRAIN: \t Epoch: 31 \t Loss: 2.4877482786784997\n",
      "TRAIN: \t Epoch: 31 \t Loss: 2.7111757950048423\n",
      "TRAIN: \t Epoch: 31 \t Loss: 3.1579892174100204\n",
      "TRAIN: \t Epoch: 31 \t Loss: 3.1042089835934\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.118336406574661\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.224629355728602\n",
      "TRAIN: \t Epoch: 31 \t Loss: 4.3547443213625305\n",
      "TRAIN: \t Epoch: 31 \t Loss: 4.532621229285515\n",
      "TRAIN: \t Epoch: 31 \t Loss: 6.105427269769618\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.97919511836804\n",
      "TRAIN: \t Epoch: 31 \t Loss: 6.258953107377228\n",
      "TRAIN: \t Epoch: 31 \t Loss: 6.281876974512647\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.503297508632992\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.491413340750789\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.134644295839481\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.144753144579265\n",
      "TRAIN: \t Epoch: 31 \t Loss: 4.580403091830823\n",
      "TRAIN: \t Epoch: 31 \t Loss: 4.377358862255159\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.443821618136221\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.548771976031386\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.668417004815607\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.592455226061131\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.6705651874433\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.69901685290374\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.5568889470201235\n",
      "TRAIN: \t Epoch: 31 \t Loss: 5.5623903109169195\n",
      "TRAIN: \t Epoch: 31 \t Loss: 4.177258446037222\n",
      "TRAIN: \t Epoch: 31 \t Loss: 4.3495157021588255\n",
      "ADE: 97.83810370731078  FDE: 23.281864517663173\n",
      "TRAIN: \t Epoch: 32 \t Loss: 2.9026025131188713\n",
      "TRAIN: \t Epoch: 32 \t Loss: 2.9430692297732395\n",
      "TRAIN: \t Epoch: 32 \t Loss: 3.637921579746173\n",
      "TRAIN: \t Epoch: 32 \t Loss: 3.6736111777687577\n",
      "TRAIN: \t Epoch: 32 \t Loss: 3.6737232955752726\n",
      "TRAIN: \t Epoch: 32 \t Loss: 3.584045685091552\n",
      "TRAIN: \t Epoch: 32 \t Loss: 4.051800355053947\n",
      "TRAIN: \t Epoch: 32 \t Loss: 3.9330487356275623\n",
      "TRAIN: \t Epoch: 32 \t Loss: 2.442508546258355\n",
      "TRAIN: \t Epoch: 32 \t Loss: 2.5916126839706815\n",
      "TRAIN: \t Epoch: 32 \t Loss: 3.1173648167925796\n",
      "TRAIN: \t Epoch: 32 \t Loss: 3.067681325474786\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.079819649010676\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.20455457215528\n",
      "TRAIN: \t Epoch: 32 \t Loss: 4.270575197710919\n",
      "TRAIN: \t Epoch: 32 \t Loss: 4.484077535595337\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.990386825429572\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.905152684221813\n",
      "TRAIN: \t Epoch: 32 \t Loss: 6.2523871987044455\n",
      "TRAIN: \t Epoch: 32 \t Loss: 6.236606042340641\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.346060350069995\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.341876371351784\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.119390589547571\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.12143595835858\n",
      "TRAIN: \t Epoch: 32 \t Loss: 4.408674820464476\n",
      "TRAIN: \t Epoch: 32 \t Loss: 4.344616636913334\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.580572598636543\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.4696711019177435\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.680862238110243\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.543581529578492\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.698899121591591\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.652881890495241\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.414164924094896\n",
      "TRAIN: \t Epoch: 32 \t Loss: 5.508190166675244\n",
      "TRAIN: \t Epoch: 32 \t Loss: 4.08919757035428\n",
      "TRAIN: \t Epoch: 32 \t Loss: 4.279971931803147\n",
      "ADE: 95.08896586960468  FDE: 23.301904467352617\n",
      "TRAIN: \t Epoch: 33 \t Loss: 2.58125330016565\n",
      "TRAIN: \t Epoch: 33 \t Loss: 2.9576736036228697\n",
      "TRAIN: \t Epoch: 33 \t Loss: 3.819768016705389\n",
      "TRAIN: \t Epoch: 33 \t Loss: 3.524253032419018\n",
      "TRAIN: \t Epoch: 33 \t Loss: 3.6221563416885694\n",
      "TRAIN: \t Epoch: 33 \t Loss: 3.6183354980175424\n",
      "TRAIN: \t Epoch: 33 \t Loss: 3.9873448502568682\n",
      "TRAIN: \t Epoch: 33 \t Loss: 3.915762538248951\n",
      "TRAIN: \t Epoch: 33 \t Loss: 2.3906197619075784\n",
      "TRAIN: \t Epoch: 33 \t Loss: 2.440530690488507\n",
      "TRAIN: \t Epoch: 33 \t Loss: 3.138105175485043\n",
      "TRAIN: \t Epoch: 33 \t Loss: 3.064650299265669\n",
      "TRAIN: \t Epoch: 33 \t Loss: 4.975443622374979\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.110532697855285\n",
      "TRAIN: \t Epoch: 33 \t Loss: 4.198657061276062\n",
      "TRAIN: \t Epoch: 33 \t Loss: 4.390773001369149\n",
      "TRAIN: \t Epoch: 33 \t Loss: 6.044773989148716\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.842790069187235\n",
      "TRAIN: \t Epoch: 33 \t Loss: 6.174620058763551\n",
      "TRAIN: \t Epoch: 33 \t Loss: 6.196015784444829\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.344088952964012\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.279115808294534\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.084805901189318\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.021789604025582\n",
      "TRAIN: \t Epoch: 33 \t Loss: 4.362551063390374\n",
      "TRAIN: \t Epoch: 33 \t Loss: 4.22236967972616\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.381112925723665\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.435416596476125\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.519575990830183\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.482534014277009\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.520962918721449\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.594369260577295\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.28489648000091\n",
      "TRAIN: \t Epoch: 33 \t Loss: 5.449106775800964\n",
      "TRAIN: \t Epoch: 33 \t Loss: 3.887345684841213\n",
      "TRAIN: \t Epoch: 33 \t Loss: 4.004849759657496\n",
      "ADE: 93.84839662850493  FDE: 23.61855320852543\n",
      "TRAIN: \t Epoch: 34 \t Loss: 2.608837060503621\n",
      "TRAIN: \t Epoch: 34 \t Loss: 2.793140523851496\n",
      "TRAIN: \t Epoch: 34 \t Loss: 3.5209981715518794\n",
      "TRAIN: \t Epoch: 34 \t Loss: 3.438525662062943\n",
      "TRAIN: \t Epoch: 34 \t Loss: 3.4281476928935963\n",
      "TRAIN: \t Epoch: 34 \t Loss: 3.4938921593915913\n",
      "TRAIN: \t Epoch: 34 \t Loss: 3.967880292519519\n",
      "TRAIN: \t Epoch: 34 \t Loss: 3.8442166805741875\n",
      "TRAIN: \t Epoch: 34 \t Loss: 2.2728302522966186\n",
      "TRAIN: \t Epoch: 34 \t Loss: 2.3947882413142376\n",
      "TRAIN: \t Epoch: 34 \t Loss: 3.2433777261989833\n",
      "TRAIN: \t Epoch: 34 \t Loss: 2.971693568981483\n",
      "TRAIN: \t Epoch: 34 \t Loss: 4.9044537296936355\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.041125562096415\n",
      "TRAIN: \t Epoch: 34 \t Loss: 4.0873105477583245\n",
      "TRAIN: \t Epoch: 34 \t Loss: 4.298631792189532\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.909188080781428\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.786243271872082\n",
      "TRAIN: \t Epoch: 34 \t Loss: 6.097652566144522\n",
      "TRAIN: \t Epoch: 34 \t Loss: 6.12998174637921\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.281244878086076\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.49664568422173\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.30031241420251\n",
      "TRAIN: \t Epoch: 34 \t Loss: 4.994895253881754\n",
      "TRAIN: \t Epoch: 34 \t Loss: 4.453779444554286\n",
      "TRAIN: \t Epoch: 34 \t Loss: 4.173802773666822\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.431059416110925\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.510899779797723\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.554406408762658\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.464096364415856\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.498353135796313\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.5956229093403325\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.318233382623956\n",
      "TRAIN: \t Epoch: 34 \t Loss: 5.4488149555419065\n",
      "TRAIN: \t Epoch: 34 \t Loss: 3.9543519851323663\n",
      "TRAIN: \t Epoch: 34 \t Loss: 4.085865122094548\n",
      "ADE: 93.85279594617067  FDE: 22.65400647396767\n",
      "TRAIN: \t Epoch: 35 \t Loss: 2.525752075849252\n",
      "TRAIN: \t Epoch: 35 \t Loss: 2.725169308158142\n",
      "TRAIN: \t Epoch: 35 \t Loss: 3.407754660302744\n",
      "TRAIN: \t Epoch: 35 \t Loss: 3.416730792571681\n",
      "TRAIN: \t Epoch: 35 \t Loss: 3.3944928732471\n",
      "TRAIN: \t Epoch: 35 \t Loss: 3.424315215848729\n",
      "TRAIN: \t Epoch: 35 \t Loss: 3.8161399105756595\n",
      "TRAIN: \t Epoch: 35 \t Loss: 3.7407092215511937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 35 \t Loss: 2.2547659722855067\n",
      "TRAIN: \t Epoch: 35 \t Loss: 2.266666799091162\n",
      "TRAIN: \t Epoch: 35 \t Loss: 3.0685397746545124\n",
      "TRAIN: \t Epoch: 35 \t Loss: 2.944005337728176\n",
      "TRAIN: \t Epoch: 35 \t Loss: 4.929768799255047\n",
      "TRAIN: \t Epoch: 35 \t Loss: 4.950717438475124\n",
      "TRAIN: \t Epoch: 35 \t Loss: 3.982562847667835\n",
      "TRAIN: \t Epoch: 35 \t Loss: 4.284342717318831\n",
      "TRAIN: \t Epoch: 35 \t Loss: 5.858436801575285\n",
      "TRAIN: \t Epoch: 35 \t Loss: 5.7418496981817855\n",
      "TRAIN: \t Epoch: 35 \t Loss: 6.128607005603792\n",
      "TRAIN: \t Epoch: 35 \t Loss: 6.1106113959237875\n",
      "TRAIN: \t Epoch: 35 \t Loss: 5.205434221733551\n",
      "TRAIN: \t Epoch: 35 \t Loss: 5.220259715883828\n",
      "TRAIN: \t Epoch: 35 \t Loss: 5.045750419478606\n",
      "TRAIN: \t Epoch: 35 \t Loss: 4.9275553830934635\n",
      "TRAIN: \t Epoch: 35 \t Loss: 4.326127414052778\n",
      "TRAIN: \t Epoch: 35 \t Loss: 6.492934787309264\n",
      "TRAIN: \t Epoch: 35 \t Loss: 6.512472061377636\n",
      "TRAIN: \t Epoch: 35 \t Loss: 6.083756234547363\n",
      "TRAIN: \t Epoch: 35 \t Loss: 6.034595032586556\n",
      "TRAIN: \t Epoch: 35 \t Loss: 6.122918989655709\n",
      "TRAIN: \t Epoch: 35 \t Loss: 6.183175536960131\n",
      "TRAIN: \t Epoch: 35 \t Loss: 6.227265651960323\n",
      "TRAIN: \t Epoch: 35 \t Loss: 5.947441075782799\n",
      "TRAIN: \t Epoch: 35 \t Loss: 5.994082375165803\n",
      "TRAIN: \t Epoch: 35 \t Loss: 5.629350953955328\n",
      "TRAIN: \t Epoch: 35 \t Loss: 5.439416424850255\n",
      "ADE: 94.06615801288072  FDE: 26.639316019972327\n",
      "TRAIN: \t Epoch: 36 \t Loss: 4.56784759820096\n",
      "TRAIN: \t Epoch: 36 \t Loss: 4.455095050302264\n",
      "TRAIN: \t Epoch: 36 \t Loss: 4.934758927527884\n",
      "TRAIN: \t Epoch: 36 \t Loss: 4.770017919988847\n",
      "TRAIN: \t Epoch: 36 \t Loss: 4.7937992932940166\n",
      "TRAIN: \t Epoch: 36 \t Loss: 4.636288146596304\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.0183101019075735\n",
      "TRAIN: \t Epoch: 36 \t Loss: 4.864490294653292\n",
      "TRAIN: \t Epoch: 36 \t Loss: 3.556107745416455\n",
      "TRAIN: \t Epoch: 36 \t Loss: 3.6493318511588826\n",
      "TRAIN: \t Epoch: 36 \t Loss: 3.9540301690957835\n",
      "TRAIN: \t Epoch: 36 \t Loss: 3.889197112654326\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.689666687349782\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.7131697263855665\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.0070173640690046\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.195025881009121\n",
      "TRAIN: \t Epoch: 36 \t Loss: 6.465103760024116\n",
      "TRAIN: \t Epoch: 36 \t Loss: 6.35824971133974\n",
      "TRAIN: \t Epoch: 36 \t Loss: 6.776320372994908\n",
      "TRAIN: \t Epoch: 36 \t Loss: 6.735478355454998\n",
      "TRAIN: \t Epoch: 36 \t Loss: 6.105085240127099\n",
      "TRAIN: \t Epoch: 36 \t Loss: 6.113792598071883\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.465176236284035\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.39609694189428\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.052147331715081\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.028853174114902\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.768903478376844\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.8216039120619705\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.864842983995996\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.917197924373972\n",
      "TRAIN: \t Epoch: 36 \t Loss: 6.1135442719477595\n",
      "TRAIN: \t Epoch: 36 \t Loss: 6.044760956605528\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.616987283617562\n",
      "TRAIN: \t Epoch: 36 \t Loss: 5.771568992605775\n",
      "TRAIN: \t Epoch: 36 \t Loss: 4.658654175616696\n",
      "TRAIN: \t Epoch: 36 \t Loss: 4.704950125933889\n",
      "ADE: 99.74167429980983  FDE: 25.214380532440607\n",
      "TRAIN: \t Epoch: 37 \t Loss: 3.471081032029892\n",
      "TRAIN: \t Epoch: 37 \t Loss: 3.3749952746370346\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.073294681708497\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.027521077738637\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.131939060837878\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.106425357098052\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.358650789744473\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.3676654031918325\n",
      "TRAIN: \t Epoch: 37 \t Loss: 2.829876345788775\n",
      "TRAIN: \t Epoch: 37 \t Loss: 2.957986855374648\n",
      "TRAIN: \t Epoch: 37 \t Loss: 3.309250458702595\n",
      "TRAIN: \t Epoch: 37 \t Loss: 3.3244487892954364\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.290548471674826\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.34715733990676\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.571224305089123\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.907438594227073\n",
      "TRAIN: \t Epoch: 37 \t Loss: 6.276562072149325\n",
      "TRAIN: \t Epoch: 37 \t Loss: 6.208145164263527\n",
      "TRAIN: \t Epoch: 37 \t Loss: 6.4800586853196425\n",
      "TRAIN: \t Epoch: 37 \t Loss: 6.56791604248474\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.853204740657454\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.880816035555928\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.247047168533803\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.208206996455969\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.83090348405511\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.766509663398995\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.572226890125038\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.655489341247173\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.748660401474928\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.732576748554801\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.79741044117463\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.902108581458815\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.484366589918782\n",
      "TRAIN: \t Epoch: 37 \t Loss: 5.593279746511412\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.461189941023379\n",
      "TRAIN: \t Epoch: 37 \t Loss: 4.578935260953669\n",
      "ADE: 97.02559732370429  FDE: 23.470415103536748\n",
      "TRAIN: \t Epoch: 38 \t Loss: 3.0698289555844163\n",
      "TRAIN: \t Epoch: 38 \t Loss: 3.0547540596172986\n",
      "TRAIN: \t Epoch: 38 \t Loss: 3.7992945034240115\n",
      "TRAIN: \t Epoch: 38 \t Loss: 3.8412802923934093\n",
      "TRAIN: \t Epoch: 38 \t Loss: 3.8631543211219026\n",
      "TRAIN: \t Epoch: 38 \t Loss: 3.8287153764366217\n",
      "TRAIN: \t Epoch: 38 \t Loss: 4.108849959401728\n",
      "TRAIN: \t Epoch: 38 \t Loss: 4.112037885945825\n",
      "TRAIN: \t Epoch: 38 \t Loss: 2.5681434685638718\n",
      "TRAIN: \t Epoch: 38 \t Loss: 2.656742978886168\n",
      "TRAIN: \t Epoch: 38 \t Loss: 3.115643419557924\n",
      "TRAIN: \t Epoch: 38 \t Loss: 3.0524930644509527\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.1241027790080516\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.179880847607298\n",
      "TRAIN: \t Epoch: 38 \t Loss: 4.4018110032899\n",
      "TRAIN: \t Epoch: 38 \t Loss: 4.683060318814902\n",
      "TRAIN: \t Epoch: 38 \t Loss: 6.136081489071728\n",
      "TRAIN: \t Epoch: 38 \t Loss: 6.022483365510071\n",
      "TRAIN: \t Epoch: 38 \t Loss: 6.27398000023112\n",
      "TRAIN: \t Epoch: 38 \t Loss: 6.356374452059876\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.66732092813662\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.680244822063413\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.043717782760439\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.063660220043888\n",
      "TRAIN: \t Epoch: 38 \t Loss: 4.647473821363281\n",
      "TRAIN: \t Epoch: 38 \t Loss: 4.611678904642961\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.534228700989024\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.567338985298848\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.660226023587939\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.5710398723429595\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.659380970950916\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.750537804398537\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.372429883311173\n",
      "TRAIN: \t Epoch: 38 \t Loss: 5.536590712583616\n",
      "TRAIN: \t Epoch: 38 \t Loss: 4.275245874418995\n",
      "TRAIN: \t Epoch: 38 \t Loss: 4.386018433855467\n",
      "ADE: 93.95649668447166  FDE: 23.760443916959773\n",
      "TRAIN: \t Epoch: 39 \t Loss: 2.8556120561641647\n",
      "TRAIN: \t Epoch: 39 \t Loss: 2.849476244763355\n",
      "TRAIN: \t Epoch: 39 \t Loss: 3.6482939599867605\n",
      "TRAIN: \t Epoch: 39 \t Loss: 3.643414214407719\n",
      "TRAIN: \t Epoch: 39 \t Loss: 3.6738035276028285\n",
      "TRAIN: \t Epoch: 39 \t Loss: 3.684024118776617\n",
      "TRAIN: \t Epoch: 39 \t Loss: 3.9784773125382853\n",
      "TRAIN: \t Epoch: 39 \t Loss: 3.9373505297078277\n",
      "TRAIN: \t Epoch: 39 \t Loss: 2.4034226478560017\n",
      "TRAIN: \t Epoch: 39 \t Loss: 2.551582798116584\n",
      "TRAIN: \t Epoch: 39 \t Loss: 3.0295718399803024\n",
      "TRAIN: \t Epoch: 39 \t Loss: 2.9366146396319053\n",
      "TRAIN: \t Epoch: 39 \t Loss: 4.990023753322008\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.138618974341613\n",
      "TRAIN: \t Epoch: 39 \t Loss: 4.267589497782906\n",
      "TRAIN: \t Epoch: 39 \t Loss: 4.495065691381643\n",
      "TRAIN: \t Epoch: 39 \t Loss: 6.011154036542405\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.866637405611842\n",
      "TRAIN: \t Epoch: 39 \t Loss: 6.198931177892151\n",
      "TRAIN: \t Epoch: 39 \t Loss: 6.2505284877955365\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.483433230840486\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.4992369723295695\n",
      "TRAIN: \t Epoch: 39 \t Loss: 4.93486597619434\n",
      "TRAIN: \t Epoch: 39 \t Loss: 4.997690641941458\n",
      "TRAIN: \t Epoch: 39 \t Loss: 4.4880892105795684\n",
      "TRAIN: \t Epoch: 39 \t Loss: 4.462005550443182\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.375477762515567\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.4443659542140965\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.539572931417298\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.48839214724243\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.562464262942535\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.699785980911596\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.279307545696807\n",
      "TRAIN: \t Epoch: 39 \t Loss: 5.4940049021987205\n",
      "TRAIN: \t Epoch: 39 \t Loss: 4.101968521496578\n",
      "TRAIN: \t Epoch: 39 \t Loss: 4.227940101791397\n",
      "ADE: 94.42811838757163  FDE: 21.982510548811124\n",
      "TRAIN: \t Epoch: 40 \t Loss: 2.673352312242142\n",
      "TRAIN: \t Epoch: 40 \t Loss: 2.7313708218036146\n",
      "TRAIN: \t Epoch: 40 \t Loss: 3.5226497337206797\n",
      "TRAIN: \t Epoch: 40 \t Loss: 3.553754729205172\n",
      "TRAIN: \t Epoch: 40 \t Loss: 3.5680383920546768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 40 \t Loss: 3.5880846105421966\n",
      "TRAIN: \t Epoch: 40 \t Loss: 3.876688118075036\n",
      "TRAIN: \t Epoch: 40 \t Loss: 3.8432648954542383\n",
      "TRAIN: \t Epoch: 40 \t Loss: 2.3321821744767446\n",
      "TRAIN: \t Epoch: 40 \t Loss: 2.4857602495852573\n",
      "TRAIN: \t Epoch: 40 \t Loss: 3.0359371907713\n",
      "TRAIN: \t Epoch: 40 \t Loss: 2.9289247784507744\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.017131280159488\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.059259612891239\n",
      "TRAIN: \t Epoch: 40 \t Loss: 4.194082160754455\n",
      "TRAIN: \t Epoch: 40 \t Loss: 4.372829892809085\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.9491996977182655\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.905539367271524\n",
      "TRAIN: \t Epoch: 40 \t Loss: 6.164405803998563\n",
      "TRAIN: \t Epoch: 40 \t Loss: 6.171468502479877\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.414891684968907\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.378808000384088\n",
      "TRAIN: \t Epoch: 40 \t Loss: 4.901102139955793\n",
      "TRAIN: \t Epoch: 40 \t Loss: 4.989384718576709\n",
      "TRAIN: \t Epoch: 40 \t Loss: 4.414004449835284\n",
      "TRAIN: \t Epoch: 40 \t Loss: 4.38872110796833\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.362112644019014\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.396134058498253\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.453597806193727\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.440021920135568\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.527923473477907\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.6215668974807045\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.275538253040792\n",
      "TRAIN: \t Epoch: 40 \t Loss: 5.445963704066408\n",
      "TRAIN: \t Epoch: 40 \t Loss: 3.96610413342065\n",
      "TRAIN: \t Epoch: 40 \t Loss: 4.110191295847696\n",
      "ADE: 92.07452051235391  FDE: 22.48488198906464\n",
      "TRAIN: \t Epoch: 41 \t Loss: 2.626807656820278\n",
      "TRAIN: \t Epoch: 41 \t Loss: 2.700100729505009\n",
      "TRAIN: \t Epoch: 41 \t Loss: 3.469118539361504\n",
      "TRAIN: \t Epoch: 41 \t Loss: 3.45987870868324\n",
      "TRAIN: \t Epoch: 41 \t Loss: 3.512820123048682\n",
      "TRAIN: \t Epoch: 41 \t Loss: 3.530226050732717\n",
      "TRAIN: \t Epoch: 41 \t Loss: 3.8363819956170144\n",
      "TRAIN: \t Epoch: 41 \t Loss: 3.831739497285194\n",
      "TRAIN: \t Epoch: 41 \t Loss: 2.296379386781175\n",
      "TRAIN: \t Epoch: 41 \t Loss: 2.4339898468031698\n",
      "TRAIN: \t Epoch: 41 \t Loss: 3.0657231979958732\n",
      "TRAIN: \t Epoch: 41 \t Loss: 2.966257669096643\n",
      "TRAIN: \t Epoch: 41 \t Loss: 4.942729991780046\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.0147480645062465\n",
      "TRAIN: \t Epoch: 41 \t Loss: 4.115141114891325\n",
      "TRAIN: \t Epoch: 41 \t Loss: 4.331212877033705\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.944425340115238\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.741798914468255\n",
      "TRAIN: \t Epoch: 41 \t Loss: 6.142779591928031\n",
      "TRAIN: \t Epoch: 41 \t Loss: 6.1216255734721035\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.273785447379652\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.291567789994996\n",
      "TRAIN: \t Epoch: 41 \t Loss: 4.891325782900574\n",
      "TRAIN: \t Epoch: 41 \t Loss: 4.98109360933635\n",
      "TRAIN: \t Epoch: 41 \t Loss: 4.3145342425965385\n",
      "TRAIN: \t Epoch: 41 \t Loss: 4.271434663804446\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.301788098853388\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.381130500229269\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.41049598682175\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.391399851589203\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.443836112483599\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.528700151236741\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.3133922857111076\n",
      "TRAIN: \t Epoch: 41 \t Loss: 5.462445439735253\n",
      "TRAIN: \t Epoch: 41 \t Loss: 3.870452947790542\n",
      "TRAIN: \t Epoch: 41 \t Loss: 4.0449833701101285\n",
      "ADE: 93.04191227398883  FDE: 21.567453796461656\n",
      "TRAIN: \t Epoch: 42 \t Loss: 2.6490176247123904\n",
      "TRAIN: \t Epoch: 42 \t Loss: 2.6483555488976545\n",
      "TRAIN: \t Epoch: 42 \t Loss: 3.407242852201672\n",
      "TRAIN: \t Epoch: 42 \t Loss: 3.4199031637210995\n",
      "TRAIN: \t Epoch: 42 \t Loss: 3.4898816401298434\n",
      "TRAIN: \t Epoch: 42 \t Loss: 3.458207336742599\n",
      "TRAIN: \t Epoch: 42 \t Loss: 3.8476995793973363\n",
      "TRAIN: \t Epoch: 42 \t Loss: 3.717849020253377\n",
      "TRAIN: \t Epoch: 42 \t Loss: 2.2690510074198387\n",
      "TRAIN: \t Epoch: 42 \t Loss: 2.410595120610715\n",
      "TRAIN: \t Epoch: 42 \t Loss: 3.088860964365207\n",
      "TRAIN: \t Epoch: 42 \t Loss: 2.9240495799776034\n",
      "TRAIN: \t Epoch: 42 \t Loss: 4.85739744817772\n",
      "TRAIN: \t Epoch: 42 \t Loss: 4.962089774089774\n",
      "TRAIN: \t Epoch: 42 \t Loss: 4.017643352714803\n",
      "TRAIN: \t Epoch: 42 \t Loss: 4.238513609352752\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.858350471358375\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.721439318304961\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.997073252617371\n",
      "TRAIN: \t Epoch: 42 \t Loss: 6.05243722552056\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.150630316276109\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.141900065701364\n",
      "TRAIN: \t Epoch: 42 \t Loss: 4.888808037154309\n",
      "TRAIN: \t Epoch: 42 \t Loss: 4.9913442730655255\n",
      "TRAIN: \t Epoch: 42 \t Loss: 4.168969890438124\n",
      "TRAIN: \t Epoch: 42 \t Loss: 4.200795040749495\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.27341454738028\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.356147488020612\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.398628144669742\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.378866899148069\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.413331313779765\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.547832275777584\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.181205140826255\n",
      "TRAIN: \t Epoch: 42 \t Loss: 5.367939931347878\n",
      "TRAIN: \t Epoch: 42 \t Loss: 3.81504956252139\n",
      "TRAIN: \t Epoch: 42 \t Loss: 3.9716176456331835\n",
      "ADE: 92.06824361826982  FDE: 21.41559103494922\n",
      "TRAIN: \t Epoch: 43 \t Loss: 2.6817259484038374\n",
      "TRAIN: \t Epoch: 43 \t Loss: 3.234504166595929\n",
      "TRAIN: \t Epoch: 43 \t Loss: 3.46139098521992\n",
      "TRAIN: \t Epoch: 43 \t Loss: 3.692379844741369\n",
      "TRAIN: \t Epoch: 43 \t Loss: 3.6075994729703975\n",
      "TRAIN: \t Epoch: 43 \t Loss: 3.5868569754430326\n",
      "TRAIN: \t Epoch: 43 \t Loss: 4.0498327663549905\n",
      "TRAIN: \t Epoch: 43 \t Loss: 3.96120448916894\n",
      "TRAIN: \t Epoch: 43 \t Loss: 2.430179752288753\n",
      "TRAIN: \t Epoch: 43 \t Loss: 2.5813666506860202\n",
      "TRAIN: \t Epoch: 43 \t Loss: 3.149466926442621\n",
      "TRAIN: \t Epoch: 43 \t Loss: 3.0543378349550108\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.007028787130668\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.070088286191732\n",
      "TRAIN: \t Epoch: 43 \t Loss: 4.111430204725471\n",
      "TRAIN: \t Epoch: 43 \t Loss: 4.33239458970406\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.879015868682572\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.783436652746241\n",
      "TRAIN: \t Epoch: 43 \t Loss: 6.187331521440727\n",
      "TRAIN: \t Epoch: 43 \t Loss: 6.182716910707208\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.334457590945243\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.293709449094273\n",
      "TRAIN: \t Epoch: 43 \t Loss: 4.8847116134150195\n",
      "TRAIN: \t Epoch: 43 \t Loss: 4.941358327020259\n",
      "TRAIN: \t Epoch: 43 \t Loss: 4.288223149760762\n",
      "TRAIN: \t Epoch: 43 \t Loss: 4.244157881650038\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.265258634001422\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.344009317969869\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.392474979098661\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.358157511809403\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.4248521706809605\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.491516672572829\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.187108460256998\n",
      "TRAIN: \t Epoch: 43 \t Loss: 5.372965200478128\n",
      "TRAIN: \t Epoch: 43 \t Loss: 3.842870289531049\n",
      "TRAIN: \t Epoch: 43 \t Loss: 3.948304649822768\n",
      "ADE: 92.30820152379076  FDE: 21.59303528975038\n",
      "TRAIN: \t Epoch: 44 \t Loss: 2.591254992311998\n",
      "TRAIN: \t Epoch: 44 \t Loss: 2.695557414109953\n",
      "TRAIN: \t Epoch: 44 \t Loss: 3.34532183729084\n",
      "TRAIN: \t Epoch: 44 \t Loss: 3.3550763024195818\n",
      "TRAIN: \t Epoch: 44 \t Loss: 3.4184255992789567\n",
      "TRAIN: \t Epoch: 44 \t Loss: 3.4491457528751868\n",
      "TRAIN: \t Epoch: 44 \t Loss: 3.811181665547549\n",
      "TRAIN: \t Epoch: 44 \t Loss: 3.7150692393313305\n",
      "TRAIN: \t Epoch: 44 \t Loss: 2.211169386998237\n",
      "TRAIN: \t Epoch: 44 \t Loss: 2.3644599853053188\n",
      "TRAIN: \t Epoch: 44 \t Loss: 3.057412301196442\n",
      "TRAIN: \t Epoch: 44 \t Loss: 2.9570027335875677\n",
      "TRAIN: \t Epoch: 44 \t Loss: 4.844069961657987\n",
      "TRAIN: \t Epoch: 44 \t Loss: 4.875295867530496\n",
      "TRAIN: \t Epoch: 44 \t Loss: 3.9716444049964337\n",
      "TRAIN: \t Epoch: 44 \t Loss: 4.197070524380123\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.751064290278824\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.651857526730308\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.988408272880857\n",
      "TRAIN: \t Epoch: 44 \t Loss: 6.047526069292466\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.152151778524354\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.140395860478403\n",
      "TRAIN: \t Epoch: 44 \t Loss: 4.780437935575387\n",
      "TRAIN: \t Epoch: 44 \t Loss: 4.883681563644425\n",
      "TRAIN: \t Epoch: 44 \t Loss: 4.125816491307529\n",
      "TRAIN: \t Epoch: 44 \t Loss: 4.131152865015332\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.175808305183785\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.270456472024154\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.344919905106926\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.27342112717179\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.300558226760444\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.445312059290733\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.082631809686499\n",
      "TRAIN: \t Epoch: 44 \t Loss: 5.286470067036072\n",
      "TRAIN: \t Epoch: 44 \t Loss: 3.7150145687681655\n",
      "TRAIN: \t Epoch: 44 \t Loss: 3.8715254357528717\n",
      "ADE: 91.52682969125165  FDE: 21.82816165974606\n",
      "TRAIN: \t Epoch: 45 \t Loss: 2.4903988660127876\n",
      "TRAIN: \t Epoch: 45 \t Loss: 2.827865982613198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 45 \t Loss: 3.8454729954019795\n",
      "TRAIN: \t Epoch: 45 \t Loss: 3.5568396805443703\n",
      "TRAIN: \t Epoch: 45 \t Loss: 3.64090570122735\n",
      "TRAIN: \t Epoch: 45 \t Loss: 3.7351560909508668\n",
      "TRAIN: \t Epoch: 45 \t Loss: 4.20896215854425\n",
      "TRAIN: \t Epoch: 45 \t Loss: 4.095331626728663\n",
      "TRAIN: \t Epoch: 45 \t Loss: 2.4265789434948433\n",
      "TRAIN: \t Epoch: 45 \t Loss: 2.519031475190281\n",
      "TRAIN: \t Epoch: 45 \t Loss: 3.06650943580141\n",
      "TRAIN: \t Epoch: 45 \t Loss: 3.0509658067566923\n",
      "TRAIN: \t Epoch: 45 \t Loss: 4.999733047584713\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.063486016114747\n",
      "TRAIN: \t Epoch: 45 \t Loss: 4.0881090679069185\n",
      "TRAIN: \t Epoch: 45 \t Loss: 4.263098254547446\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.7586104028452905\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.749442797937001\n",
      "TRAIN: \t Epoch: 45 \t Loss: 6.102593598456968\n",
      "TRAIN: \t Epoch: 45 \t Loss: 6.129412800195036\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.26022828648231\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.230692567584899\n",
      "TRAIN: \t Epoch: 45 \t Loss: 4.786108750286816\n",
      "TRAIN: \t Epoch: 45 \t Loss: 4.908581697145974\n",
      "TRAIN: \t Epoch: 45 \t Loss: 4.275151618718899\n",
      "TRAIN: \t Epoch: 45 \t Loss: 4.146472379512938\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.274896252263071\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.356503719924153\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.396949989929063\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.349316368718201\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.337712631976181\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.468422054486219\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.143810788443526\n",
      "TRAIN: \t Epoch: 45 \t Loss: 5.3382560989508905\n",
      "TRAIN: \t Epoch: 45 \t Loss: 3.797347035708447\n",
      "TRAIN: \t Epoch: 45 \t Loss: 3.908660110367177\n",
      "ADE: 92.75505333055465  FDE: 21.295758525420894\n",
      "TRAIN: \t Epoch: 46 \t Loss: 2.5120062785567665\n",
      "TRAIN: \t Epoch: 46 \t Loss: 2.6230583888349637\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.3313033646715366\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.2998204524329586\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.3578407836303943\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.36883836813864\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.7174099293835874\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.7240837260113953\n",
      "TRAIN: \t Epoch: 46 \t Loss: 2.1716718993463098\n",
      "TRAIN: \t Epoch: 46 \t Loss: 2.240757659945557\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.0322609077959752\n",
      "TRAIN: \t Epoch: 46 \t Loss: 2.8534490360824414\n",
      "TRAIN: \t Epoch: 46 \t Loss: 4.782199900254604\n",
      "TRAIN: \t Epoch: 46 \t Loss: 4.851088779969853\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.890250576533\n",
      "TRAIN: \t Epoch: 46 \t Loss: 4.104451808724621\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.7263463729130315\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.589743237375231\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.942271610877801\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.978628269654824\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.089951815513183\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.0887010095098795\n",
      "TRAIN: \t Epoch: 46 \t Loss: 4.7313489364158094\n",
      "TRAIN: \t Epoch: 46 \t Loss: 4.785568532443851\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.9981471115685747\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.972985741193365\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.142354156884743\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.226946124597252\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.275720584057341\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.234286795972474\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.22061476966539\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.324623539826511\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.037656807074927\n",
      "TRAIN: \t Epoch: 46 \t Loss: 5.196510537402621\n",
      "TRAIN: \t Epoch: 46 \t Loss: 3.605376275936005\n",
      "TRAIN: \t Epoch: 46 \t Loss: 4.104459365048137\n",
      "ADE: 90.51724816983231  FDE: 21.119741144290956\n",
      "TRAIN: \t Epoch: 47 \t Loss: 8.420925141934251\n",
      "TRAIN: \t Epoch: 47 \t Loss: 8.033616027161127\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.785429021479775\n",
      "TRAIN: \t Epoch: 47 \t Loss: 4.071766504661842\n",
      "TRAIN: \t Epoch: 47 \t Loss: 4.710760615943244\n",
      "TRAIN: \t Epoch: 47 \t Loss: 4.996822287333011\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.579967968168349\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.544126988057445\n",
      "TRAIN: \t Epoch: 47 \t Loss: 4.680825392173125\n",
      "TRAIN: \t Epoch: 47 \t Loss: 4.725885774794376\n",
      "TRAIN: \t Epoch: 47 \t Loss: 4.933153884945325\n",
      "TRAIN: \t Epoch: 47 \t Loss: 4.90305367666978\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.835579548124199\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.850641257709306\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.48734341095659\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.468396027324156\n",
      "TRAIN: \t Epoch: 47 \t Loss: 6.790875611542846\n",
      "TRAIN: \t Epoch: 47 \t Loss: 6.637944808993628\n",
      "TRAIN: \t Epoch: 47 \t Loss: 6.730940246226767\n",
      "TRAIN: \t Epoch: 47 \t Loss: 6.769406470176934\n",
      "TRAIN: \t Epoch: 47 \t Loss: 6.412016572418023\n",
      "TRAIN: \t Epoch: 47 \t Loss: 6.41785326910851\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.747079237213349\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.661690905711584\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.410391505648552\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.326054274360774\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.933671978349193\n",
      "TRAIN: \t Epoch: 47 \t Loss: 6.032579869773501\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.9295238098044365\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.920655551102719\n",
      "TRAIN: \t Epoch: 47 \t Loss: 6.110258849527648\n",
      "TRAIN: \t Epoch: 47 \t Loss: 6.095861236606908\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.764480114450413\n",
      "TRAIN: \t Epoch: 47 \t Loss: 5.758457409008677\n",
      "TRAIN: \t Epoch: 47 \t Loss: 4.827362423655639\n",
      "TRAIN: \t Epoch: 47 \t Loss: 4.900095281733962\n",
      "ADE: 92.56658252917843  FDE: 23.857980735666068\n",
      "TRAIN: \t Epoch: 48 \t Loss: 3.9361322206300215\n",
      "TRAIN: \t Epoch: 48 \t Loss: 3.8300529098125202\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.3775311303805475\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.353958237975377\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.305686845584163\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.2613934536770275\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.727194357809726\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.608710859345885\n",
      "TRAIN: \t Epoch: 48 \t Loss: 3.278155900747497\n",
      "TRAIN: \t Epoch: 48 \t Loss: 3.370783449277533\n",
      "TRAIN: \t Epoch: 48 \t Loss: 3.63204688171484\n",
      "TRAIN: \t Epoch: 48 \t Loss: 3.541384172459878\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.340735866594675\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.398272521380592\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.735853881050446\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.95759346915249\n",
      "TRAIN: \t Epoch: 48 \t Loss: 6.28763005632585\n",
      "TRAIN: \t Epoch: 48 \t Loss: 6.363394744841683\n",
      "TRAIN: \t Epoch: 48 \t Loss: 6.440387092297379\n",
      "TRAIN: \t Epoch: 48 \t Loss: 6.435070110082073\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.870694782361536\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.930294170331888\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.2362617575955905\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.2062732555481634\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.812509188338112\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.818395658223907\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.638227785555317\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.739774733485424\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.610456915708353\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.638521888625769\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.743584197821725\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.8388951706155305\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.446171773868042\n",
      "TRAIN: \t Epoch: 48 \t Loss: 5.55322773632326\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.479809488132307\n",
      "TRAIN: \t Epoch: 48 \t Loss: 4.470892898373674\n",
      "ADE: 92.08738983930103  FDE: 22.41285286207124\n",
      "TRAIN: \t Epoch: 49 \t Loss: 3.145064445928353\n",
      "TRAIN: \t Epoch: 49 \t Loss: 3.1482515483298528\n",
      "TRAIN: \t Epoch: 49 \t Loss: 3.8973979629374664\n",
      "TRAIN: \t Epoch: 49 \t Loss: 3.8939676520686417\n",
      "TRAIN: \t Epoch: 49 \t Loss: 3.8169003722869252\n",
      "TRAIN: \t Epoch: 49 \t Loss: 3.834984490370808\n",
      "TRAIN: \t Epoch: 49 \t Loss: 4.185800154533929\n",
      "TRAIN: \t Epoch: 49 \t Loss: 4.152015669873934\n",
      "TRAIN: \t Epoch: 49 \t Loss: 2.664500263743454\n",
      "TRAIN: \t Epoch: 49 \t Loss: 2.7899507648401234\n",
      "TRAIN: \t Epoch: 49 \t Loss: 3.242371950928012\n",
      "TRAIN: \t Epoch: 49 \t Loss: 3.1650118036260713\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.079202161978172\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.146254639489676\n",
      "TRAIN: \t Epoch: 49 \t Loss: 4.380586932132391\n",
      "TRAIN: \t Epoch: 49 \t Loss: 4.620135419876092\n",
      "TRAIN: \t Epoch: 49 \t Loss: 6.026042372825548\n",
      "TRAIN: \t Epoch: 49 \t Loss: 6.052957294345186\n",
      "TRAIN: \t Epoch: 49 \t Loss: 6.244567560017929\n",
      "TRAIN: \t Epoch: 49 \t Loss: 6.287906428608882\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.641906864754439\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.665057577601293\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.021832712221693\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.039448614774376\n",
      "TRAIN: \t Epoch: 49 \t Loss: 4.59409734075944\n",
      "TRAIN: \t Epoch: 49 \t Loss: 4.549944525482387\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.38614538008359\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.533607753339746\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.45239408934247\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.538344196871239\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.626360992560877\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.692938049382449\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.337850288936869\n",
      "TRAIN: \t Epoch: 49 \t Loss: 5.449806162462813\n",
      "TRAIN: \t Epoch: 49 \t Loss: 4.235365072117261\n",
      "TRAIN: \t Epoch: 49 \t Loss: 4.228582613771914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADE: 91.32723095989262  FDE: 22.34939843299844\n",
      "TRAIN: \t Epoch: 50 \t Loss: 2.8676264833931695\n",
      "TRAIN: \t Epoch: 50 \t Loss: 2.940758117585187\n",
      "TRAIN: \t Epoch: 50 \t Loss: 3.6748996869560964\n",
      "TRAIN: \t Epoch: 50 \t Loss: 3.6347494457041294\n",
      "TRAIN: \t Epoch: 50 \t Loss: 3.594058706885601\n",
      "TRAIN: \t Epoch: 50 \t Loss: 3.5973687988840344\n",
      "TRAIN: \t Epoch: 50 \t Loss: 4.009687343747505\n",
      "TRAIN: \t Epoch: 50 \t Loss: 3.9506402248821817\n",
      "TRAIN: \t Epoch: 50 \t Loss: 2.3729796732738837\n",
      "TRAIN: \t Epoch: 50 \t Loss: 2.532921686434058\n",
      "TRAIN: \t Epoch: 50 \t Loss: 3.1268786980701044\n",
      "TRAIN: \t Epoch: 50 \t Loss: 3.1008322154656103\n",
      "TRAIN: \t Epoch: 50 \t Loss: 4.984071553048568\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.07535772861992\n",
      "TRAIN: \t Epoch: 50 \t Loss: 4.271831666104172\n",
      "TRAIN: \t Epoch: 50 \t Loss: 4.482431323274055\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.959573460808658\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.870960032012508\n",
      "TRAIN: \t Epoch: 50 \t Loss: 6.1502224275138175\n",
      "TRAIN: \t Epoch: 50 \t Loss: 6.235635604962427\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.531978321390598\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.543650258415392\n",
      "TRAIN: \t Epoch: 50 \t Loss: 4.953458170522779\n",
      "TRAIN: \t Epoch: 50 \t Loss: 4.918050983448448\n",
      "TRAIN: \t Epoch: 50 \t Loss: 4.464749036851414\n",
      "TRAIN: \t Epoch: 50 \t Loss: 4.461706848025781\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.327054038870552\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.481836416744265\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.51974644429346\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.4628598713505365\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.54598805642592\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.635363716180603\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.46541905536132\n",
      "TRAIN: \t Epoch: 50 \t Loss: 5.452604663871783\n",
      "TRAIN: \t Epoch: 50 \t Loss: 4.117900401938359\n",
      "TRAIN: \t Epoch: 50 \t Loss: 4.200488851600812\n",
      "ADE: 91.01808797036782  FDE: 22.93754714661805\n",
      "TRAIN: \t Epoch: 51 \t Loss: 2.7631921977582476\n",
      "TRAIN: \t Epoch: 51 \t Loss: 2.8961499978090375\n",
      "TRAIN: \t Epoch: 51 \t Loss: 3.5748279324671572\n",
      "TRAIN: \t Epoch: 51 \t Loss: 3.5297205956620776\n",
      "TRAIN: \t Epoch: 51 \t Loss: 3.4999978721019502\n",
      "TRAIN: \t Epoch: 51 \t Loss: 3.549236973965243\n",
      "TRAIN: \t Epoch: 51 \t Loss: 3.938752050124354\n",
      "TRAIN: \t Epoch: 51 \t Loss: 3.8601310529486943\n",
      "TRAIN: \t Epoch: 51 \t Loss: 2.283459316693269\n",
      "TRAIN: \t Epoch: 51 \t Loss: 2.4788868599646428\n",
      "TRAIN: \t Epoch: 51 \t Loss: 3.057349386112457\n",
      "TRAIN: \t Epoch: 51 \t Loss: 3.030867801216591\n",
      "TRAIN: \t Epoch: 51 \t Loss: 4.918831966823238\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.0828793662524525\n",
      "TRAIN: \t Epoch: 51 \t Loss: 4.186624732509173\n",
      "TRAIN: \t Epoch: 51 \t Loss: 4.361724403120301\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.8450406077179\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.807101427310318\n",
      "TRAIN: \t Epoch: 51 \t Loss: 6.082582475018602\n",
      "TRAIN: \t Epoch: 51 \t Loss: 6.1234238526003475\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.4229398752532845\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.380270437376346\n",
      "TRAIN: \t Epoch: 51 \t Loss: 4.8143284264166955\n",
      "TRAIN: \t Epoch: 51 \t Loss: 4.874266876441192\n",
      "TRAIN: \t Epoch: 51 \t Loss: 4.285583685712422\n",
      "TRAIN: \t Epoch: 51 \t Loss: 4.254646653257776\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.234527291261434\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.343526431145648\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.368720605190149\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.3168994021577545\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.369619360043254\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.514813302299708\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.165119352132685\n",
      "TRAIN: \t Epoch: 51 \t Loss: 5.344155307143274\n",
      "TRAIN: \t Epoch: 51 \t Loss: 3.907802142349113\n",
      "TRAIN: \t Epoch: 51 \t Loss: 4.0226124986348495\n",
      "ADE: 90.06035063000823  FDE: 21.127006935544415\n",
      "TRAIN: \t Epoch: 52 \t Loss: 2.696898131197381\n",
      "TRAIN: \t Epoch: 52 \t Loss: 2.9165978284964362\n",
      "TRAIN: \t Epoch: 52 \t Loss: 3.4429899304050062\n",
      "TRAIN: \t Epoch: 52 \t Loss: 3.3938809234430503\n",
      "TRAIN: \t Epoch: 52 \t Loss: 3.538864333496286\n",
      "TRAIN: \t Epoch: 52 \t Loss: 3.495072537226326\n",
      "TRAIN: \t Epoch: 52 \t Loss: 3.8450692588826243\n",
      "TRAIN: \t Epoch: 52 \t Loss: 3.811009911274075\n",
      "TRAIN: \t Epoch: 52 \t Loss: 2.234547616389819\n",
      "TRAIN: \t Epoch: 52 \t Loss: 2.420386987720179\n",
      "TRAIN: \t Epoch: 52 \t Loss: 3.0796083800947547\n",
      "TRAIN: \t Epoch: 52 \t Loss: 2.9215421586631045\n",
      "TRAIN: \t Epoch: 52 \t Loss: 4.860848558774482\n",
      "TRAIN: \t Epoch: 52 \t Loss: 4.989234478952441\n",
      "TRAIN: \t Epoch: 52 \t Loss: 4.122099031967156\n",
      "TRAIN: \t Epoch: 52 \t Loss: 4.312317944124218\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.810411775089477\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.730136552753064\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.99885610526492\n",
      "TRAIN: \t Epoch: 52 \t Loss: 6.06988347820046\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.301774267340091\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.29102746156005\n",
      "TRAIN: \t Epoch: 52 \t Loss: 4.789076741179784\n",
      "TRAIN: \t Epoch: 52 \t Loss: 4.921183916100727\n",
      "TRAIN: \t Epoch: 52 \t Loss: 4.21027992302539\n",
      "TRAIN: \t Epoch: 52 \t Loss: 4.213805396418977\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.2246053463114475\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.3082921176469275\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.361899642806303\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.384640096661867\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.327179386612108\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.459734909595916\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.137455219622453\n",
      "TRAIN: \t Epoch: 52 \t Loss: 5.348684022368625\n",
      "TRAIN: \t Epoch: 52 \t Loss: 3.847847493958558\n",
      "TRAIN: \t Epoch: 52 \t Loss: 3.9669549060895775\n",
      "ADE: 89.6161414076341  FDE: 20.570302823836705\n",
      "TRAIN: \t Epoch: 53 \t Loss: 2.6606983599665814\n",
      "TRAIN: \t Epoch: 53 \t Loss: 2.7645102735796887\n",
      "TRAIN: \t Epoch: 53 \t Loss: 3.372956529407346\n",
      "TRAIN: \t Epoch: 53 \t Loss: 3.427263246759855\n",
      "TRAIN: \t Epoch: 53 \t Loss: 3.4793934200576295\n",
      "TRAIN: \t Epoch: 53 \t Loss: 3.434006546116801\n",
      "TRAIN: \t Epoch: 53 \t Loss: 3.821185807541674\n",
      "TRAIN: \t Epoch: 53 \t Loss: 3.765330037345008\n",
      "TRAIN: \t Epoch: 53 \t Loss: 2.1440541891047236\n",
      "TRAIN: \t Epoch: 53 \t Loss: 2.3138351270884234\n",
      "TRAIN: \t Epoch: 53 \t Loss: 3.1660998523472417\n",
      "TRAIN: \t Epoch: 53 \t Loss: 2.983043672576215\n",
      "TRAIN: \t Epoch: 53 \t Loss: 4.832942722563491\n",
      "TRAIN: \t Epoch: 53 \t Loss: 4.9456299158690715\n",
      "TRAIN: \t Epoch: 53 \t Loss: 4.062134132988933\n",
      "TRAIN: \t Epoch: 53 \t Loss: 4.288956257770965\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.78650598437373\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.69505691037183\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.995646063048611\n",
      "TRAIN: \t Epoch: 53 \t Loss: 6.0150229589114375\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.1509637323096875\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.185469555987352\n",
      "TRAIN: \t Epoch: 53 \t Loss: 4.774156256454091\n",
      "TRAIN: \t Epoch: 53 \t Loss: 4.921130944987302\n",
      "TRAIN: \t Epoch: 53 \t Loss: 4.11354742158891\n",
      "TRAIN: \t Epoch: 53 \t Loss: 4.159204182483245\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.254954351940869\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.294929400404715\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.351321527518376\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.3502818666170135\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.334431510390668\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.522161060803804\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.079459852346683\n",
      "TRAIN: \t Epoch: 53 \t Loss: 5.282779571228985\n",
      "TRAIN: \t Epoch: 53 \t Loss: 3.963036204499586\n",
      "TRAIN: \t Epoch: 53 \t Loss: 4.015998046718804\n",
      "ADE: 91.41548722801457  FDE: 21.64526328680673\n",
      "TRAIN: \t Epoch: 54 \t Loss: 2.460804862191502\n",
      "TRAIN: \t Epoch: 54 \t Loss: 2.70242746456322\n",
      "TRAIN: \t Epoch: 54 \t Loss: 3.317313879076777\n",
      "TRAIN: \t Epoch: 54 \t Loss: 3.3344649475445336\n",
      "TRAIN: \t Epoch: 54 \t Loss: 3.4124694630922012\n",
      "TRAIN: \t Epoch: 54 \t Loss: 3.339612458547665\n",
      "TRAIN: \t Epoch: 54 \t Loss: 3.749840416858612\n",
      "TRAIN: \t Epoch: 54 \t Loss: 3.687667129263897\n",
      "TRAIN: \t Epoch: 54 \t Loss: 2.1392465664012708\n",
      "TRAIN: \t Epoch: 54 \t Loss: 2.26889041300607\n",
      "TRAIN: \t Epoch: 54 \t Loss: 2.942252333026897\n",
      "TRAIN: \t Epoch: 54 \t Loss: 2.887903015559625\n",
      "TRAIN: \t Epoch: 54 \t Loss: 4.740030576710234\n",
      "TRAIN: \t Epoch: 54 \t Loss: 4.880592016764243\n",
      "TRAIN: \t Epoch: 54 \t Loss: 3.947866424965353\n",
      "TRAIN: \t Epoch: 54 \t Loss: 4.157157128751485\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.737686591508778\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.688849541937619\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.930830481455102\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.994707426647274\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.151674080624414\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.164665708767142\n",
      "TRAIN: \t Epoch: 54 \t Loss: 4.743283332698621\n",
      "TRAIN: \t Epoch: 54 \t Loss: 4.84482691807567\n",
      "TRAIN: \t Epoch: 54 \t Loss: 4.037812160219554\n",
      "TRAIN: \t Epoch: 54 \t Loss: 4.046610143352802\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.131856620120782\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.251512269097221\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.292033577916114\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.212842062994958\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.210119509603105\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.383616098534217\n",
      "TRAIN: \t Epoch: 54 \t Loss: 5.0197969000971145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 54 \t Loss: 5.222930281452051\n",
      "TRAIN: \t Epoch: 54 \t Loss: 3.679325993444594\n",
      "TRAIN: \t Epoch: 54 \t Loss: 3.808258167842034\n",
      "ADE: 88.17193864618031  FDE: 21.435916791966907\n",
      "TRAIN: \t Epoch: 55 \t Loss: 2.7235570174491976\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.913149657896448\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.8867157072905645\n",
      "TRAIN: \t Epoch: 55 \t Loss: 4.530179114686998\n",
      "TRAIN: \t Epoch: 55 \t Loss: 4.932952879713207\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.238409827108862\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.548862503073306\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.545375707995516\n",
      "TRAIN: \t Epoch: 55 \t Loss: 4.789381268503249\n",
      "TRAIN: \t Epoch: 55 \t Loss: 4.8125890632850705\n",
      "TRAIN: \t Epoch: 55 \t Loss: 4.981103578139453\n",
      "TRAIN: \t Epoch: 55 \t Loss: 4.887766403152184\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.972124572763809\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.072990255392982\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.565921600306086\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.7158160714160635\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.89933718463858\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.679755597674243\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.823445502936708\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.856730342114383\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.539204749348077\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.569496321723676\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.893688715079314\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.870990405822514\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.5149983017650515\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.441411986617505\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.91677431616039\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.011182232963977\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.971575017015363\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.947430389494509\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.122583785202774\n",
      "TRAIN: \t Epoch: 55 \t Loss: 6.16487443354309\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.830066322960629\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.806813318301775\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.082878559371002\n",
      "TRAIN: \t Epoch: 55 \t Loss: 5.050180306037008\n",
      "ADE: 93.55653945371748  FDE: 24.95752840427009\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.140633960668634\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.070827130631905\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.569417124011256\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.561408619799136\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.577928408449783\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.552952826021558\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.765685241563286\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.730832892942332\n",
      "TRAIN: \t Epoch: 56 \t Loss: 3.4101608873363682\n",
      "TRAIN: \t Epoch: 56 \t Loss: 3.4839395002259472\n",
      "TRAIN: \t Epoch: 56 \t Loss: 3.769088638777884\n",
      "TRAIN: \t Epoch: 56 \t Loss: 3.747558178095407\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.386090269368003\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.414052505308478\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.814533140984306\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.12277983299625\n",
      "TRAIN: \t Epoch: 56 \t Loss: 6.333864030125495\n",
      "TRAIN: \t Epoch: 56 \t Loss: 6.3247927967469675\n",
      "TRAIN: \t Epoch: 56 \t Loss: 6.536025568778244\n",
      "TRAIN: \t Epoch: 56 \t Loss: 6.590244454097421\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.987656942220934\n",
      "TRAIN: \t Epoch: 56 \t Loss: 6.022085905137629\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.366055080969142\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.29351497596162\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.8294586981344345\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.81427467641317\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.4882349319496235\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.598509964952396\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.719791526962145\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.685373438553007\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.737834433988446\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.882969753414779\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.459002903542385\n",
      "TRAIN: \t Epoch: 56 \t Loss: 5.677352450661383\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.560171803493491\n",
      "TRAIN: \t Epoch: 56 \t Loss: 4.586827706933431\n",
      "ADE: 92.6765339446467  FDE: 22.431790426443598\n",
      "TRAIN: \t Epoch: 57 \t Loss: 3.300696870239264\n",
      "TRAIN: \t Epoch: 57 \t Loss: 3.2280859211952193\n",
      "TRAIN: \t Epoch: 57 \t Loss: 3.9382257221951753\n",
      "TRAIN: \t Epoch: 57 \t Loss: 3.9572135500390204\n",
      "TRAIN: \t Epoch: 57 \t Loss: 4.021242938849537\n",
      "TRAIN: \t Epoch: 57 \t Loss: 4.003000544301635\n",
      "TRAIN: \t Epoch: 57 \t Loss: 4.22972356873215\n",
      "TRAIN: \t Epoch: 57 \t Loss: 4.194455128976329\n",
      "TRAIN: \t Epoch: 57 \t Loss: 2.74925692309664\n",
      "TRAIN: \t Epoch: 57 \t Loss: 2.841593576563796\n",
      "TRAIN: \t Epoch: 57 \t Loss: 3.20400619684273\n",
      "TRAIN: \t Epoch: 57 \t Loss: 3.1777094691585246\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.089055067552144\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.193744457791145\n",
      "TRAIN: \t Epoch: 57 \t Loss: 4.43077231876109\n",
      "TRAIN: \t Epoch: 57 \t Loss: 4.794966588680652\n",
      "TRAIN: \t Epoch: 57 \t Loss: 6.13743013839767\n",
      "TRAIN: \t Epoch: 57 \t Loss: 6.106759235720432\n",
      "TRAIN: \t Epoch: 57 \t Loss: 6.296876951747544\n",
      "TRAIN: \t Epoch: 57 \t Loss: 6.407565637858463\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.7356148831059866\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.776662855544842\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.146466208738587\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.107644791518942\n",
      "TRAIN: \t Epoch: 57 \t Loss: 4.634853838035207\n",
      "TRAIN: \t Epoch: 57 \t Loss: 4.545179185239637\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.384248955655901\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.520668749460681\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.56470965317958\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.581551734157489\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.6339682861631895\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.779748623482401\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.354981500529927\n",
      "TRAIN: \t Epoch: 57 \t Loss: 5.56870239940788\n",
      "TRAIN: \t Epoch: 57 \t Loss: 4.375333782119808\n",
      "TRAIN: \t Epoch: 57 \t Loss: 4.431132369101119\n",
      "ADE: 89.85244790010745  FDE: 22.838184923139973\n",
      "TRAIN: \t Epoch: 58 \t Loss: 2.9607213104792556\n",
      "TRAIN: \t Epoch: 58 \t Loss: 2.9084408450618464\n",
      "TRAIN: \t Epoch: 58 \t Loss: 3.7460994460192754\n",
      "TRAIN: \t Epoch: 58 \t Loss: 3.732224035992115\n",
      "TRAIN: \t Epoch: 58 \t Loss: 3.8151263627493313\n",
      "TRAIN: \t Epoch: 58 \t Loss: 3.7665051609302673\n",
      "TRAIN: \t Epoch: 58 \t Loss: 3.979613617242252\n",
      "TRAIN: \t Epoch: 58 \t Loss: 3.9703513847377585\n",
      "TRAIN: \t Epoch: 58 \t Loss: 2.524379069752322\n",
      "TRAIN: \t Epoch: 58 \t Loss: 2.529278456800342\n",
      "TRAIN: \t Epoch: 58 \t Loss: 3.027996018906014\n",
      "TRAIN: \t Epoch: 58 \t Loss: 3.075414741966842\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.015675916761646\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.046707391790131\n",
      "TRAIN: \t Epoch: 58 \t Loss: 4.325839000471275\n",
      "TRAIN: \t Epoch: 58 \t Loss: 4.654901885660082\n",
      "TRAIN: \t Epoch: 58 \t Loss: 6.026260593874629\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.963252730821542\n",
      "TRAIN: \t Epoch: 58 \t Loss: 6.146054747093126\n",
      "TRAIN: \t Epoch: 58 \t Loss: 6.272802284891094\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.574861459073022\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.6226223137328235\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.061466238938022\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.030575965526387\n",
      "TRAIN: \t Epoch: 58 \t Loss: 4.523220594267395\n",
      "TRAIN: \t Epoch: 58 \t Loss: 4.492172215662504\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.3253728298393925\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.484873936942789\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.503691475152458\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.48506891184401\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.5827582419672614\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.736076927320773\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.350838514153865\n",
      "TRAIN: \t Epoch: 58 \t Loss: 5.462443368901195\n",
      "TRAIN: \t Epoch: 58 \t Loss: 4.312877141055433\n",
      "TRAIN: \t Epoch: 58 \t Loss: 4.436315686045192\n",
      "ADE: 89.61657145903862  FDE: 21.602886356575176\n",
      "TRAIN: \t Epoch: 59 \t Loss: 2.8157335291642123\n",
      "TRAIN: \t Epoch: 59 \t Loss: 2.771993220923325\n",
      "TRAIN: \t Epoch: 59 \t Loss: 3.623850811286708\n",
      "TRAIN: \t Epoch: 59 \t Loss: 3.6296605869751377\n",
      "TRAIN: \t Epoch: 59 \t Loss: 3.727459765105249\n",
      "TRAIN: \t Epoch: 59 \t Loss: 3.724984133675722\n",
      "TRAIN: \t Epoch: 59 \t Loss: 3.888952587450257\n",
      "TRAIN: \t Epoch: 59 \t Loss: 3.8947198811523167\n",
      "TRAIN: \t Epoch: 59 \t Loss: 2.388373926509926\n",
      "TRAIN: \t Epoch: 59 \t Loss: 2.438186507632654\n",
      "TRAIN: \t Epoch: 59 \t Loss: 2.9560975869757944\n",
      "TRAIN: \t Epoch: 59 \t Loss: 2.9944360647643022\n",
      "TRAIN: \t Epoch: 59 \t Loss: 4.976223994425606\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.045118626073145\n",
      "TRAIN: \t Epoch: 59 \t Loss: 4.22459310034935\n",
      "TRAIN: \t Epoch: 59 \t Loss: 4.559454613376782\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.8647733616680355\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.920564406714078\n",
      "TRAIN: \t Epoch: 59 \t Loss: 6.110851930204754\n",
      "TRAIN: \t Epoch: 59 \t Loss: 6.191517500031102\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.507073732420189\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.574947844248276\n",
      "TRAIN: \t Epoch: 59 \t Loss: 4.98918032768314\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.017537977031178\n",
      "TRAIN: \t Epoch: 59 \t Loss: 4.4531184491343385\n",
      "TRAIN: \t Epoch: 59 \t Loss: 4.429042104572817\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.31601549158774\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.410636417785506\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.509203614754889\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.54312389069023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 59 \t Loss: 5.545190271386897\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.606094673840112\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.263646242319835\n",
      "TRAIN: \t Epoch: 59 \t Loss: 5.404852887465779\n",
      "TRAIN: \t Epoch: 59 \t Loss: 4.280925982120456\n",
      "TRAIN: \t Epoch: 59 \t Loss: 4.391033271994302\n",
      "ADE: 89.63649325640642  FDE: 21.496124385395966\n",
      "TRAIN: \t Epoch: 60 \t Loss: 2.7379957959939762\n",
      "TRAIN: \t Epoch: 60 \t Loss: 2.688717536359354\n",
      "TRAIN: \t Epoch: 60 \t Loss: 3.565141440656951\n",
      "TRAIN: \t Epoch: 60 \t Loss: 3.575258398805175\n",
      "TRAIN: \t Epoch: 60 \t Loss: 3.628653072012322\n",
      "TRAIN: \t Epoch: 60 \t Loss: 3.6301766569552587\n",
      "TRAIN: \t Epoch: 60 \t Loss: 3.830735787335532\n",
      "TRAIN: \t Epoch: 60 \t Loss: 3.8414883889459666\n",
      "TRAIN: \t Epoch: 60 \t Loss: 2.3448686288453575\n",
      "TRAIN: \t Epoch: 60 \t Loss: 2.4001720639118416\n",
      "TRAIN: \t Epoch: 60 \t Loss: 2.919429062607716\n",
      "TRAIN: \t Epoch: 60 \t Loss: 2.9107631257572826\n",
      "TRAIN: \t Epoch: 60 \t Loss: 4.926645396073965\n",
      "TRAIN: \t Epoch: 60 \t Loss: 4.999108529470757\n",
      "TRAIN: \t Epoch: 60 \t Loss: 4.18156290083805\n",
      "TRAIN: \t Epoch: 60 \t Loss: 4.471206078991827\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.8011259818184\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.831751858523695\n",
      "TRAIN: \t Epoch: 60 \t Loss: 6.060826857243246\n",
      "TRAIN: \t Epoch: 60 \t Loss: 6.142126017137908\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.427862704946188\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.497800218415623\n",
      "TRAIN: \t Epoch: 60 \t Loss: 4.887328730236331\n",
      "TRAIN: \t Epoch: 60 \t Loss: 4.937708812625452\n",
      "TRAIN: \t Epoch: 60 \t Loss: 4.387585059376247\n",
      "TRAIN: \t Epoch: 60 \t Loss: 4.387698440553038\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.238423903717955\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.370605619944826\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.415214396923333\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.390461951106798\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.521239425466567\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.575788369405118\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.201457447216888\n",
      "TRAIN: \t Epoch: 60 \t Loss: 5.324441598495343\n",
      "TRAIN: \t Epoch: 60 \t Loss: 4.11566260852793\n",
      "TRAIN: \t Epoch: 60 \t Loss: 4.28872398227753\n",
      "ADE: 87.98263507382481  FDE: 21.875717510256692\n",
      "TRAIN: \t Epoch: 61 \t Loss: 2.671159108987857\n",
      "TRAIN: \t Epoch: 61 \t Loss: 2.601920994396994\n",
      "TRAIN: \t Epoch: 61 \t Loss: 3.5439238429296434\n",
      "TRAIN: \t Epoch: 61 \t Loss: 3.532841133563795\n",
      "TRAIN: \t Epoch: 61 \t Loss: 3.5276410700843273\n",
      "TRAIN: \t Epoch: 61 \t Loss: 3.563152586134226\n",
      "TRAIN: \t Epoch: 61 \t Loss: 3.763532386863995\n",
      "TRAIN: \t Epoch: 61 \t Loss: 3.7899920544689176\n",
      "TRAIN: \t Epoch: 61 \t Loss: 2.2828480986729023\n",
      "TRAIN: \t Epoch: 61 \t Loss: 2.3508535207110306\n",
      "TRAIN: \t Epoch: 61 \t Loss: 2.896907283654558\n",
      "TRAIN: \t Epoch: 61 \t Loss: 2.8573042917195846\n",
      "TRAIN: \t Epoch: 61 \t Loss: 4.864347560917114\n",
      "TRAIN: \t Epoch: 61 \t Loss: 4.906805455976416\n",
      "TRAIN: \t Epoch: 61 \t Loss: 4.142220171558346\n",
      "TRAIN: \t Epoch: 61 \t Loss: 4.388497425048343\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.765303002180808\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.7746718479510175\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.984774374856059\n",
      "TRAIN: \t Epoch: 61 \t Loss: 6.072090050562019\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.332507510906087\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.384330209025139\n",
      "TRAIN: \t Epoch: 61 \t Loss: 4.828220972247571\n",
      "TRAIN: \t Epoch: 61 \t Loss: 4.881821281511398\n",
      "TRAIN: \t Epoch: 61 \t Loss: 4.28503105927682\n",
      "TRAIN: \t Epoch: 61 \t Loss: 4.2903879177583235\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.2088019600754265\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.3018820712473325\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.365336136584455\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.448366716631149\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.434020850234066\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.544353098098385\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.194852623880687\n",
      "TRAIN: \t Epoch: 61 \t Loss: 5.341118350158484\n",
      "TRAIN: \t Epoch: 61 \t Loss: 4.1562170368597\n",
      "TRAIN: \t Epoch: 61 \t Loss: 4.2600882702351806\n",
      "ADE: 87.00629749810284  FDE: 20.88058757351771\n",
      "TRAIN: \t Epoch: 62 \t Loss: 2.5730083972444486\n",
      "TRAIN: \t Epoch: 62 \t Loss: 2.575821001649239\n",
      "TRAIN: \t Epoch: 62 \t Loss: 3.501773951034076\n",
      "TRAIN: \t Epoch: 62 \t Loss: 3.4857519191635693\n",
      "TRAIN: \t Epoch: 62 \t Loss: 3.4547910354593676\n",
      "TRAIN: \t Epoch: 62 \t Loss: 3.527490041509868\n",
      "TRAIN: \t Epoch: 62 \t Loss: 3.739480565481314\n",
      "TRAIN: \t Epoch: 62 \t Loss: 3.718415331277082\n",
      "TRAIN: \t Epoch: 62 \t Loss: 2.314791882393979\n",
      "TRAIN: \t Epoch: 62 \t Loss: 2.3283933913535586\n",
      "TRAIN: \t Epoch: 62 \t Loss: 2.9252681470792607\n",
      "TRAIN: \t Epoch: 62 \t Loss: 2.852740416139567\n",
      "TRAIN: \t Epoch: 62 \t Loss: 4.819927691391589\n",
      "TRAIN: \t Epoch: 62 \t Loss: 4.885362676315001\n",
      "TRAIN: \t Epoch: 62 \t Loss: 4.069780748880575\n",
      "TRAIN: \t Epoch: 62 \t Loss: 4.306359632026359\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.6954726655399215\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.742237964748066\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.9436919683425256\n",
      "TRAIN: \t Epoch: 62 \t Loss: 6.008256755227098\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.281156213316948\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.3176034981109055\n",
      "TRAIN: \t Epoch: 62 \t Loss: 4.816538790725394\n",
      "TRAIN: \t Epoch: 62 \t Loss: 4.836646074302353\n",
      "TRAIN: \t Epoch: 62 \t Loss: 4.173210286064202\n",
      "TRAIN: \t Epoch: 62 \t Loss: 4.188310419925564\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.134242264195935\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.248101314038841\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.308880445068693\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.286168077345917\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.342769698616833\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.497185385934033\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.108213321523293\n",
      "TRAIN: \t Epoch: 62 \t Loss: 5.275266215798079\n",
      "TRAIN: \t Epoch: 62 \t Loss: 3.9671684892753265\n",
      "TRAIN: \t Epoch: 62 \t Loss: 4.100838649737898\n",
      "ADE: 87.01379242779171  FDE: 20.1462609644676\n",
      "TRAIN: \t Epoch: 63 \t Loss: 2.5060035529448905\n",
      "TRAIN: \t Epoch: 63 \t Loss: 2.5386360200457285\n",
      "TRAIN: \t Epoch: 63 \t Loss: 3.420670062799754\n",
      "TRAIN: \t Epoch: 63 \t Loss: 3.4417264541174606\n",
      "TRAIN: \t Epoch: 63 \t Loss: 3.431261977845172\n",
      "TRAIN: \t Epoch: 63 \t Loss: 3.4932549783140154\n",
      "TRAIN: \t Epoch: 63 \t Loss: 3.714451733390662\n",
      "TRAIN: \t Epoch: 63 \t Loss: 3.698796460460904\n",
      "TRAIN: \t Epoch: 63 \t Loss: 2.2126056181725255\n",
      "TRAIN: \t Epoch: 63 \t Loss: 2.3535701267057547\n",
      "TRAIN: \t Epoch: 63 \t Loss: 2.9401611498846445\n",
      "TRAIN: \t Epoch: 63 \t Loss: 2.8356586370098595\n",
      "TRAIN: \t Epoch: 63 \t Loss: 4.736680015432907\n",
      "TRAIN: \t Epoch: 63 \t Loss: 4.835918328088099\n",
      "TRAIN: \t Epoch: 63 \t Loss: 4.012285438151772\n",
      "TRAIN: \t Epoch: 63 \t Loss: 4.218488296001222\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.678955846592069\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.706897949543304\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.916098055873627\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.9633659110602775\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.185916667054718\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.20279694179868\n",
      "TRAIN: \t Epoch: 63 \t Loss: 4.793306811443852\n",
      "TRAIN: \t Epoch: 63 \t Loss: 4.812621488307168\n",
      "TRAIN: \t Epoch: 63 \t Loss: 4.124391594002159\n",
      "TRAIN: \t Epoch: 63 \t Loss: 4.138395534759747\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.073359407462544\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.167813095541861\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.215389757261404\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.261706258462696\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.293339952749642\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.468460805759494\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.050323839163804\n",
      "TRAIN: \t Epoch: 63 \t Loss: 5.217375516261954\n",
      "TRAIN: \t Epoch: 63 \t Loss: 3.820615288629296\n",
      "TRAIN: \t Epoch: 63 \t Loss: 3.9688526454808555\n",
      "ADE: 85.9998071841305  FDE: 20.445672292323085\n",
      "TRAIN: \t Epoch: 64 \t Loss: 2.450591399396537\n",
      "TRAIN: \t Epoch: 64 \t Loss: 2.525520765558761\n",
      "TRAIN: \t Epoch: 64 \t Loss: 3.3732836257171255\n",
      "TRAIN: \t Epoch: 64 \t Loss: 3.3679972137936405\n",
      "TRAIN: \t Epoch: 64 \t Loss: 3.370100338333213\n",
      "TRAIN: \t Epoch: 64 \t Loss: 3.4047541320699235\n",
      "TRAIN: \t Epoch: 64 \t Loss: 3.667768567329433\n",
      "TRAIN: \t Epoch: 64 \t Loss: 3.625774824399097\n",
      "TRAIN: \t Epoch: 64 \t Loss: 2.1424215135136113\n",
      "TRAIN: \t Epoch: 64 \t Loss: 2.2361373521484915\n",
      "TRAIN: \t Epoch: 64 \t Loss: 2.8853628955785404\n",
      "TRAIN: \t Epoch: 64 \t Loss: 2.826361373773871\n",
      "TRAIN: \t Epoch: 64 \t Loss: 4.7092251605337605\n",
      "TRAIN: \t Epoch: 64 \t Loss: 4.8366476519121315\n",
      "TRAIN: \t Epoch: 64 \t Loss: 3.918852653420034\n",
      "TRAIN: \t Epoch: 64 \t Loss: 4.149111168164444\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.64544971250028\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.623382205251973\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.853271056856798\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.926463951560175\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.125375327923482\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.148130805754533\n",
      "TRAIN: \t Epoch: 64 \t Loss: 4.743265121375442\n",
      "TRAIN: \t Epoch: 64 \t Loss: 4.795653484777581\n",
      "TRAIN: \t Epoch: 64 \t Loss: 4.03623141435922\n",
      "TRAIN: \t Epoch: 64 \t Loss: 4.09170125861712\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.028439679013726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 64 \t Loss: 5.168004570617978\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.169732603998892\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.239687139180404\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.277333937970253\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.365762448218061\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.0447420623780825\n",
      "TRAIN: \t Epoch: 64 \t Loss: 5.193002164761936\n",
      "TRAIN: \t Epoch: 64 \t Loss: 3.774990462895523\n",
      "TRAIN: \t Epoch: 64 \t Loss: 3.8605695116668683\n",
      "ADE: 83.77351143089653  FDE: 20.661962197878445\n",
      "TRAIN: \t Epoch: 65 \t Loss: 2.391349212374796\n",
      "TRAIN: \t Epoch: 65 \t Loss: 2.540027743827867\n",
      "TRAIN: \t Epoch: 65 \t Loss: 3.7287087349321624\n",
      "TRAIN: \t Epoch: 65 \t Loss: 3.6044844179622326\n",
      "TRAIN: \t Epoch: 65 \t Loss: 3.3406353941170126\n",
      "TRAIN: \t Epoch: 65 \t Loss: 3.5545985058829315\n",
      "TRAIN: \t Epoch: 65 \t Loss: 3.793088440293621\n",
      "TRAIN: \t Epoch: 65 \t Loss: 3.7475711708835298\n",
      "TRAIN: \t Epoch: 65 \t Loss: 2.246655191047405\n",
      "TRAIN: \t Epoch: 65 \t Loss: 2.2900583911149752\n",
      "TRAIN: \t Epoch: 65 \t Loss: 2.940845568239396\n",
      "TRAIN: \t Epoch: 65 \t Loss: 2.9212930155766035\n",
      "TRAIN: \t Epoch: 65 \t Loss: 4.706220452879681\n",
      "TRAIN: \t Epoch: 65 \t Loss: 4.839384503244039\n",
      "TRAIN: \t Epoch: 65 \t Loss: 3.9803077960473785\n",
      "TRAIN: \t Epoch: 65 \t Loss: 4.159349387557652\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.669195563635057\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.6641275361151315\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.880587515512308\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.944558585067902\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.158871294370282\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.185431439400796\n",
      "TRAIN: \t Epoch: 65 \t Loss: 4.692739405372654\n",
      "TRAIN: \t Epoch: 65 \t Loss: 4.7826480127286874\n",
      "TRAIN: \t Epoch: 65 \t Loss: 4.062334837110322\n",
      "TRAIN: \t Epoch: 65 \t Loss: 4.039728413764781\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.058078116990371\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.126336915018154\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.182067615137526\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.185553270668431\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.202777378711239\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.385981762549792\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.004398276893339\n",
      "TRAIN: \t Epoch: 65 \t Loss: 5.175273335789068\n",
      "TRAIN: \t Epoch: 65 \t Loss: 3.7607445738938647\n",
      "TRAIN: \t Epoch: 65 \t Loss: 3.884539433946339\n",
      "ADE: 83.71394086758308  FDE: 21.13851805487675\n",
      "TRAIN: \t Epoch: 66 \t Loss: 2.3477423181397623\n",
      "TRAIN: \t Epoch: 66 \t Loss: 2.4612523499663292\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.264800096973764\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.3460870396217226\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.2335439797934056\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.346507096049001\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.629764958975329\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.595478711411587\n",
      "TRAIN: \t Epoch: 66 \t Loss: 2.091195987148739\n",
      "TRAIN: \t Epoch: 66 \t Loss: 2.180520823963879\n",
      "TRAIN: \t Epoch: 66 \t Loss: 2.8747918695646115\n",
      "TRAIN: \t Epoch: 66 \t Loss: 2.740919467462052\n",
      "TRAIN: \t Epoch: 66 \t Loss: 4.6566156212514835\n",
      "TRAIN: \t Epoch: 66 \t Loss: 4.708654241343876\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.8055229105300845\n",
      "TRAIN: \t Epoch: 66 \t Loss: 4.047327973464214\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.5296818071372025\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.580666873922152\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.8053413660746145\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.8039259711507585\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.030014072914486\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.033153033966405\n",
      "TRAIN: \t Epoch: 66 \t Loss: 4.628087581235432\n",
      "TRAIN: \t Epoch: 66 \t Loss: 4.744538492881354\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.9186863867651054\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.99561383096199\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.019806724576998\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.118089962555885\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.080727791584169\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.172028224690538\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.158363818142631\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.324428317147257\n",
      "TRAIN: \t Epoch: 66 \t Loss: 4.951312494552865\n",
      "TRAIN: \t Epoch: 66 \t Loss: 5.140405283678462\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.7510624812999263\n",
      "TRAIN: \t Epoch: 66 \t Loss: 3.9411306760534597\n",
      "ADE: 82.09137793940599  FDE: 19.945679439774977\n",
      "TRAIN: \t Epoch: 67 \t Loss: 2.5568895767933326\n",
      "TRAIN: \t Epoch: 67 \t Loss: 3.0306824825891203\n",
      "TRAIN: \t Epoch: 67 \t Loss: 3.956105418311101\n",
      "TRAIN: \t Epoch: 67 \t Loss: 3.412972159114833\n",
      "TRAIN: \t Epoch: 67 \t Loss: 3.319669389440348\n",
      "TRAIN: \t Epoch: 67 \t Loss: 3.5956924611929844\n",
      "TRAIN: \t Epoch: 67 \t Loss: 3.9672186736818356\n",
      "TRAIN: \t Epoch: 67 \t Loss: 3.974720457523943\n",
      "TRAIN: \t Epoch: 67 \t Loss: 2.4752341632606587\n",
      "TRAIN: \t Epoch: 67 \t Loss: 2.509458715514798\n",
      "TRAIN: \t Epoch: 67 \t Loss: 3.0276104935674444\n",
      "TRAIN: \t Epoch: 67 \t Loss: 2.9416873693697596\n",
      "TRAIN: \t Epoch: 67 \t Loss: 4.776772339344481\n",
      "TRAIN: \t Epoch: 67 \t Loss: 4.89445703634739\n",
      "TRAIN: \t Epoch: 67 \t Loss: 4.091305058921959\n",
      "TRAIN: \t Epoch: 67 \t Loss: 4.192921441862601\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.635096788074087\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.667530193121701\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.930854995375839\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.9505143706794925\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.234857359141818\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.28983027927708\n",
      "TRAIN: \t Epoch: 67 \t Loss: 4.684298929522067\n",
      "TRAIN: \t Epoch: 67 \t Loss: 4.7572224018644675\n",
      "TRAIN: \t Epoch: 67 \t Loss: 4.1094255803019974\n",
      "TRAIN: \t Epoch: 67 \t Loss: 4.057901612866997\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.082280244689327\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.197459297965059\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.1797122024733335\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.176111714954598\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.201457676458367\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.353644239790614\n",
      "TRAIN: \t Epoch: 67 \t Loss: 4.961292768177108\n",
      "TRAIN: \t Epoch: 67 \t Loss: 5.113882788399139\n",
      "TRAIN: \t Epoch: 67 \t Loss: 3.8038674126460754\n",
      "TRAIN: \t Epoch: 67 \t Loss: 3.846646137627953\n",
      "ADE: 84.29152024612223  FDE: 19.65760445543037\n",
      "TRAIN: \t Epoch: 68 \t Loss: 2.3687765746378724\n",
      "TRAIN: \t Epoch: 68 \t Loss: 2.4928183685048593\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.2533877745711113\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.292581448297597\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.1662806497525318\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.3167616358891476\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.5234306171800207\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.5002310220076707\n",
      "TRAIN: \t Epoch: 68 \t Loss: 2.065505803938896\n",
      "TRAIN: \t Epoch: 68 \t Loss: 2.1213762907507068\n",
      "TRAIN: \t Epoch: 68 \t Loss: 2.852103033392248\n",
      "TRAIN: \t Epoch: 68 \t Loss: 2.711370324632951\n",
      "TRAIN: \t Epoch: 68 \t Loss: 4.575557070618599\n",
      "TRAIN: \t Epoch: 68 \t Loss: 4.618571979585679\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.7930005345862594\n",
      "TRAIN: \t Epoch: 68 \t Loss: 4.003822737158198\n",
      "TRAIN: \t Epoch: 68 \t Loss: 5.502505779026576\n",
      "TRAIN: \t Epoch: 68 \t Loss: 5.477953535774491\n",
      "TRAIN: \t Epoch: 68 \t Loss: 5.757823830658833\n",
      "TRAIN: \t Epoch: 68 \t Loss: 5.772425429630928\n",
      "TRAIN: \t Epoch: 68 \t Loss: 4.983414862970908\n",
      "TRAIN: \t Epoch: 68 \t Loss: 4.996900242675985\n",
      "TRAIN: \t Epoch: 68 \t Loss: 4.616924524392909\n",
      "TRAIN: \t Epoch: 68 \t Loss: 4.694647024282792\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.8533721605316344\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.9282374183398407\n",
      "TRAIN: \t Epoch: 68 \t Loss: 4.957123523414122\n",
      "TRAIN: \t Epoch: 68 \t Loss: 5.060352527142095\n",
      "TRAIN: \t Epoch: 68 \t Loss: 5.083497074957234\n",
      "TRAIN: \t Epoch: 68 \t Loss: 5.110810003694077\n",
      "TRAIN: \t Epoch: 68 \t Loss: 5.119107221866706\n",
      "TRAIN: \t Epoch: 68 \t Loss: 5.248889421269168\n",
      "TRAIN: \t Epoch: 68 \t Loss: 4.884106930377383\n",
      "TRAIN: \t Epoch: 68 \t Loss: 5.107152065261626\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.580640081314167\n",
      "TRAIN: \t Epoch: 68 \t Loss: 3.745263874729571\n",
      "ADE: 82.84400522922914  FDE: 19.67670276897441\n",
      "TRAIN: \t Epoch: 69 \t Loss: 2.3294595476480424\n",
      "TRAIN: \t Epoch: 69 \t Loss: 2.5320045053368685\n",
      "TRAIN: \t Epoch: 69 \t Loss: 4.000012198909978\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.568714987482009\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.413520165547191\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.378833622614704\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.9208919252542187\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.8713260778314273\n",
      "TRAIN: \t Epoch: 69 \t Loss: 2.2654113204210304\n",
      "TRAIN: \t Epoch: 69 \t Loss: 2.373621296630625\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.055184409316934\n",
      "TRAIN: \t Epoch: 69 \t Loss: 2.908176584329054\n",
      "TRAIN: \t Epoch: 69 \t Loss: 4.694578959124673\n",
      "TRAIN: \t Epoch: 69 \t Loss: 4.764642992297557\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.9990007010488986\n",
      "TRAIN: \t Epoch: 69 \t Loss: 4.114142505731878\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.579017332642282\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.588430399873428\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.879636417349376\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.910781645539361\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.111590250982889\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.176170103640869\n",
      "TRAIN: \t Epoch: 69 \t Loss: 4.691551527933759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 69 \t Loss: 4.751628535121606\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.993761192779168\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.986757570018599\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.015666514607831\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.10358481807462\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.11832905299771\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.157713920511767\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.10825865770805\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.290865027181759\n",
      "TRAIN: \t Epoch: 69 \t Loss: 4.9039392457797675\n",
      "TRAIN: \t Epoch: 69 \t Loss: 5.1003072601243025\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.621944048736823\n",
      "TRAIN: \t Epoch: 69 \t Loss: 3.723398645672115\n",
      "ADE: 81.3831795269448  FDE: 20.59158924906979\n",
      "TRAIN: \t Epoch: 70 \t Loss: 2.23429997898866\n",
      "TRAIN: \t Epoch: 70 \t Loss: 2.331245564576839\n",
      "TRAIN: \t Epoch: 70 \t Loss: 3.1363136280504746\n",
      "TRAIN: \t Epoch: 70 \t Loss: 3.2456175409153363\n",
      "TRAIN: \t Epoch: 70 \t Loss: 3.0818314927332127\n",
      "TRAIN: \t Epoch: 70 \t Loss: 3.2350419694082313\n",
      "TRAIN: \t Epoch: 70 \t Loss: 3.416996317343004\n",
      "TRAIN: \t Epoch: 70 \t Loss: 3.4086381577435882\n",
      "TRAIN: \t Epoch: 70 \t Loss: 1.9475047675426902\n",
      "TRAIN: \t Epoch: 70 \t Loss: 2.0660201049037865\n",
      "TRAIN: \t Epoch: 70 \t Loss: 2.8172484910486175\n",
      "TRAIN: \t Epoch: 70 \t Loss: 2.706258368075959\n",
      "TRAIN: \t Epoch: 70 \t Loss: 4.515337699265161\n",
      "TRAIN: \t Epoch: 70 \t Loss: 4.50867795223548\n",
      "TRAIN: \t Epoch: 70 \t Loss: 3.683723062066116\n",
      "TRAIN: \t Epoch: 70 \t Loss: 3.928603459330615\n",
      "TRAIN: \t Epoch: 70 \t Loss: 5.468216363105796\n",
      "TRAIN: \t Epoch: 70 \t Loss: 5.444797300988164\n",
      "TRAIN: \t Epoch: 70 \t Loss: 5.7421456269638345\n",
      "TRAIN: \t Epoch: 70 \t Loss: 5.7120024635831586\n",
      "TRAIN: \t Epoch: 70 \t Loss: 4.869785261049245\n",
      "TRAIN: \t Epoch: 70 \t Loss: 4.939910708107615\n",
      "TRAIN: \t Epoch: 70 \t Loss: 4.800195911190885\n",
      "TRAIN: \t Epoch: 70 \t Loss: 4.845679652443471\n",
      "TRAIN: \t Epoch: 70 \t Loss: 3.864560322659356\n",
      "TRAIN: \t Epoch: 70 \t Loss: 4.679682244809656\n",
      "TRAIN: \t Epoch: 70 \t Loss: 6.391787463407175\n",
      "TRAIN: \t Epoch: 70 \t Loss: 6.740235062626155\n",
      "TRAIN: \t Epoch: 70 \t Loss: 6.496263427515127\n",
      "TRAIN: \t Epoch: 70 \t Loss: 5.7255779678164345\n",
      "TRAIN: \t Epoch: 70 \t Loss: 5.447951739887635\n",
      "TRAIN: \t Epoch: 70 \t Loss: 5.586842611942348\n",
      "TRAIN: \t Epoch: 70 \t Loss: 5.276838974540276\n",
      "TRAIN: \t Epoch: 70 \t Loss: 5.43778415305108\n",
      "TRAIN: \t Epoch: 70 \t Loss: 4.608589517824371\n",
      "TRAIN: \t Epoch: 70 \t Loss: 4.5419054632459535\n",
      "ADE: 79.56854752043125  FDE: 21.59709446876309\n",
      "TRAIN: \t Epoch: 71 \t Loss: 3.7231849753385444\n",
      "TRAIN: \t Epoch: 71 \t Loss: 3.614363752640477\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.116874821114807\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.093826547823565\n",
      "TRAIN: \t Epoch: 71 \t Loss: 3.9786769238987594\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.008670790333269\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.2761649335430585\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.21347402817574\n",
      "TRAIN: \t Epoch: 71 \t Loss: 2.7740465902415057\n",
      "TRAIN: \t Epoch: 71 \t Loss: 2.831686247679271\n",
      "TRAIN: \t Epoch: 71 \t Loss: 3.1452174597722253\n",
      "TRAIN: \t Epoch: 71 \t Loss: 3.0502274910703964\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.920453216831069\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.031012250266234\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.209056106810633\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.447589449256069\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.755915176345084\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.896387033858938\n",
      "TRAIN: \t Epoch: 71 \t Loss: 6.000385340987814\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.977683396874019\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.345421989237011\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.393742510124177\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.766078508639399\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.764491442130229\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.219963493129283\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.229397028443694\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.0675785106801605\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.185671965422479\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.154044997326476\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.165686269452526\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.246110316988488\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.385641937470458\n",
      "TRAIN: \t Epoch: 71 \t Loss: 4.912871513387915\n",
      "TRAIN: \t Epoch: 71 \t Loss: 5.213927759867742\n",
      "TRAIN: \t Epoch: 71 \t Loss: 3.982916374520812\n",
      "TRAIN: \t Epoch: 71 \t Loss: 3.987109392199266\n",
      "ADE: 80.58781733467094  FDE: 20.170739274125733\n",
      "TRAIN: \t Epoch: 72 \t Loss: 2.6714516508900084\n",
      "TRAIN: \t Epoch: 72 \t Loss: 2.549610799961132\n",
      "TRAIN: \t Epoch: 72 \t Loss: 3.438570433738932\n",
      "TRAIN: \t Epoch: 72 \t Loss: 3.4180864171486682\n",
      "TRAIN: \t Epoch: 72 \t Loss: 3.337107674980266\n",
      "TRAIN: \t Epoch: 72 \t Loss: 3.415453250098169\n",
      "TRAIN: \t Epoch: 72 \t Loss: 3.677906687349496\n",
      "TRAIN: \t Epoch: 72 \t Loss: 3.6685275089276312\n",
      "TRAIN: \t Epoch: 72 \t Loss: 2.10879730889846\n",
      "TRAIN: \t Epoch: 72 \t Loss: 2.310987457684372\n",
      "TRAIN: \t Epoch: 72 \t Loss: 2.80952472395246\n",
      "TRAIN: \t Epoch: 72 \t Loss: 2.764842856453537\n",
      "TRAIN: \t Epoch: 72 \t Loss: 4.7104534771718205\n",
      "TRAIN: \t Epoch: 72 \t Loss: 4.682275584609275\n",
      "TRAIN: \t Epoch: 72 \t Loss: 3.961575300875253\n",
      "TRAIN: \t Epoch: 72 \t Loss: 4.223469462215999\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.6170962858750375\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.587317403315884\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.847443186429506\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.861303755156669\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.177115740919357\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.192205575450865\n",
      "TRAIN: \t Epoch: 72 \t Loss: 4.630627045043212\n",
      "TRAIN: \t Epoch: 72 \t Loss: 4.680077865724356\n",
      "TRAIN: \t Epoch: 72 \t Loss: 4.0242501170091485\n",
      "TRAIN: \t Epoch: 72 \t Loss: 4.050308599163731\n",
      "TRAIN: \t Epoch: 72 \t Loss: 4.965309788290664\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.094878880511221\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.106952494744509\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.114317591573366\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.175205360971866\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.317598232570777\n",
      "TRAIN: \t Epoch: 72 \t Loss: 4.829883324906403\n",
      "TRAIN: \t Epoch: 72 \t Loss: 5.141802585258639\n",
      "TRAIN: \t Epoch: 72 \t Loss: 3.742788134413592\n",
      "TRAIN: \t Epoch: 72 \t Loss: 3.8581540215718717\n",
      "ADE: 81.72473034274594  FDE: 19.652599361481656\n",
      "TRAIN: \t Epoch: 73 \t Loss: 2.381456884945181\n",
      "TRAIN: \t Epoch: 73 \t Loss: 2.4116766588556398\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.275433955460614\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.26181417923841\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.2151417119286174\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.2357981614888667\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.5188200630674067\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.522250845103171\n",
      "TRAIN: \t Epoch: 73 \t Loss: 1.9593448677755512\n",
      "TRAIN: \t Epoch: 73 \t Loss: 2.1236821840512015\n",
      "TRAIN: \t Epoch: 73 \t Loss: 2.8067431318933735\n",
      "TRAIN: \t Epoch: 73 \t Loss: 2.6997338390379335\n",
      "TRAIN: \t Epoch: 73 \t Loss: 4.569951857459096\n",
      "TRAIN: \t Epoch: 73 \t Loss: 4.589363287609648\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.811609718204019\n",
      "TRAIN: \t Epoch: 73 \t Loss: 4.064730097563733\n",
      "TRAIN: \t Epoch: 73 \t Loss: 5.54537424441987\n",
      "TRAIN: \t Epoch: 73 \t Loss: 5.545334303980214\n",
      "TRAIN: \t Epoch: 73 \t Loss: 5.750571402430394\n",
      "TRAIN: \t Epoch: 73 \t Loss: 5.748408587114868\n",
      "TRAIN: \t Epoch: 73 \t Loss: 5.058637513131559\n",
      "TRAIN: \t Epoch: 73 \t Loss: 5.071311497119103\n",
      "TRAIN: \t Epoch: 73 \t Loss: 4.560364877324227\n",
      "TRAIN: \t Epoch: 73 \t Loss: 4.6096669974740845\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.883147599507489\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.876635936556032\n",
      "TRAIN: \t Epoch: 73 \t Loss: 4.895859949245814\n",
      "TRAIN: \t Epoch: 73 \t Loss: 4.991889276867121\n",
      "TRAIN: \t Epoch: 73 \t Loss: 4.982532201989657\n",
      "TRAIN: \t Epoch: 73 \t Loss: 4.9940173910857215\n",
      "TRAIN: \t Epoch: 73 \t Loss: 5.07551483680316\n",
      "TRAIN: \t Epoch: 73 \t Loss: 5.215103380880662\n",
      "TRAIN: \t Epoch: 73 \t Loss: 4.792703290406033\n",
      "TRAIN: \t Epoch: 73 \t Loss: 5.053567333139311\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.561019384817451\n",
      "TRAIN: \t Epoch: 73 \t Loss: 3.6802539320573784\n",
      "ADE: 78.85584100504505  FDE: 18.998804788800996\n",
      "TRAIN: \t Epoch: 74 \t Loss: 2.264130072890456\n",
      "TRAIN: \t Epoch: 74 \t Loss: 2.303339410761976\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.1588233540269846\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.164491050156942\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.0825716567922177\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.1697482982110623\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.4623894482268747\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.4418793347950896\n",
      "TRAIN: \t Epoch: 74 \t Loss: 1.905811653028222\n",
      "TRAIN: \t Epoch: 74 \t Loss: 2.055614088486927\n",
      "TRAIN: \t Epoch: 74 \t Loss: 2.7604441823208563\n",
      "TRAIN: \t Epoch: 74 \t Loss: 2.677065340195014\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.4764130117288445\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.475291460014469\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.6640160092609535\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.958996723150749\n",
      "TRAIN: \t Epoch: 74 \t Loss: 5.450613708770707\n",
      "TRAIN: \t Epoch: 74 \t Loss: 5.490637461579226\n",
      "TRAIN: \t Epoch: 74 \t Loss: 5.69117321527887\n",
      "TRAIN: \t Epoch: 74 \t Loss: 5.691212954322182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \t Epoch: 74 \t Loss: 4.924378881322987\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.975713328805202\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.560748135163775\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.6163374021646835\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.785031134713503\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.790609082204317\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.835586657412545\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.925033869774498\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.905856100066987\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.956499830769064\n",
      "TRAIN: \t Epoch: 74 \t Loss: 5.005944397205821\n",
      "TRAIN: \t Epoch: 74 \t Loss: 5.144340167169082\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.720022451334191\n",
      "TRAIN: \t Epoch: 74 \t Loss: 4.993606115656205\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.4375035447360127\n",
      "TRAIN: \t Epoch: 74 \t Loss: 3.5494220634763636\n",
      "ADE: 78.31220175912551  FDE: 19.178861694051765\n",
      "TRAIN: \t Epoch: 75 \t Loss: 2.198455671307765\n",
      "TRAIN: \t Epoch: 75 \t Loss: 2.3342582231875744\n",
      "TRAIN: \t Epoch: 75 \t Loss: 3.243642843791543\n",
      "TRAIN: \t Epoch: 75 \t Loss: 3.225868301623131\n",
      "TRAIN: \t Epoch: 75 \t Loss: 3.0522906949247024\n",
      "TRAIN: \t Epoch: 75 \t Loss: 3.1542018977088686\n",
      "TRAIN: \t Epoch: 75 \t Loss: 3.5011238904473863\n",
      "TRAIN: \t Epoch: 75 \t Loss: 3.4761267291253657\n",
      "TRAIN: \t Epoch: 75 \t Loss: 1.941842397735041\n",
      "TRAIN: \t Epoch: 75 \t Loss: 2.0971893734645377\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train(train_dataset, epoch)\n",
    "\n",
    "    if epoch > 20:\n",
    "        ade_ = 99999\n",
    "        fde_ = 99999\n",
    "        ad, fd = test(test_dataset, 20)\n",
    "        ade_new = min(ade_, ad)\n",
    "        fde_new = min(fde_, fd)\n",
    "\n",
    "        if ade_new < ade_ and fde_new < fde_:\n",
    "            ade_ = ade_new\n",
    "            fde_ = fde_new\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(\n",
    "                    args.checkpoint,\n",
    "                    \"val_best_{}_{}_{}.pth\".format(\n",
    "                        epoch, ade_.item() * 5.0, fde_.item() * 5.0\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        print(\"ADE:\", ade_.item() * 5.0, \" FDE:\", fde_.item() * 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f73392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
